\documentclass[a4paper,11pt,twoside]{report}
% KOMPILOWAĆ ZA POMOCĄ pdfLaTeXa, PRZEZ XeLaTeXa MOŻE NIE BYĆ POLSKICH ZNAKÓW

% -------------- Kodowanie znaków, język polski -------------

\usepackage[utf8]{inputenc}
\usepackage[MeX]{polski}
\usepackage[T1]{fontenc}
\usepackage[english,polish]{babel}


\usepackage{amsmath, amsfonts, amsthm, latexsym} % głównie symbole matematyczne, środowiska twierdzeń

\usepackage[final]{pdfpages} % inputowanie pdfa
%\usepackage[backend=bibtex, style=verbose-trad2]{biblatex}


% ---------------- Marginesy, akapity, interlinia ------------------

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry}


\linespread{1.5}
\allowdisplaybreaks

\usepackage{indentfirst} % opcjonalnie; pierwszy akapit z wcięciem
\setlength{\parindent}{5mm}


%--------------------------- ŻYWA PAGINA ------------------------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
% numery stron: lewa do lewego, prawa do prawego 
\fancyfoot[LE,RO]{\thepage} 
% prawa pagina: zawartość \rightmark do lewego, wewnętrznego (marginesu) 
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
% lewa pagina: zawartość \leftmark do prawego, wewnętrznego (marginesu) 
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{
\markboth{\thechapter.\ #1}{}}

% kreski oddzielające paginy (górną i dolną):
\renewcommand{\headrulewidth}{0 pt} % 0 - nie ma, 0.5 - jest linia


\fancypagestyle{plain}{% to definiuje wygląd pierwszej strony nowego rozdziału - obecnie tylko numeracja
  \fancyhf{}%
  \fancyfoot[LE,RO]{\thepage}%
  
  \renewcommand{\headrulewidth}{0pt}% Line at the header invisible
  \renewcommand{\footrulewidth}{0.0pt}
}



% ---------------- Nagłówki rozdziałów ---------------------

\usepackage{titlesec}
\titleformat{\chapter}%[display]
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt} 
%\titlespacing{\co}{akapit}{ile przed}{ile po} 
    
\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% ----------------------- Spis treści ---------------------------
\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


% kropki dla chapterów
\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}% <section-type>
  [0pt]% <left>
  {}% <above-code>
  {\bfseries \thecontentslabel.\quad}% <numbered-entry-format>
  {\bfseries}% <numberless-entry-format>
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}% <filler-page-format>

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% ---------------------- Spisy tabel i obrazków ----------------------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}
%\let\c@table\c@figure % jeśli włączone, numeruje tabele i obrazki razem


% --------------------- Definicje, twierdzenia etc. ---------------


\makeatletter
\newtheoremstyle{definition}%    % Name
{3ex}%                          % Space above
{3ex}%                          % Space below
{\upshape}%                      % Body font
{}%                              % Indent amount
{\bfseries}%                     % Theorem head font
{.}%                             % Punctuation after theorem head
{.5em}%                            % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%  % Theorem head spec (can be left empty, meaning `normal')
\makeatother

% ----------------------------- POLSKI --------------------------------

\theoremstyle{definition}
\newtheorem{theorem}{Twierdzenie}[chapter]
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{example}[theorem]{Przykład}
\newtheorem{proposition}[theorem]{Stwierdzenie}
\newtheorem{corollary}[theorem]{Wniosek}
\newtheorem{definition}[theorem]{Definicja}
\newtheorem{remark}[theorem]{Uwaga}



% ----------------------------- Dowód -----------------------------

%\makeatletter
%\renewenvironment{proof}[1][\proofname]
%{\par
%  \vspace{-12pt}% remove the space after the theorem
%  \pushQED{\qed}%
%  \normalfont
%  \topsep0pt \partopsep0pt % no space before
%  \trivlist
%  \item[\hskip\labelsep
%        \sc
%    #1\@addpunct{:}]\ignorespaces
%}
%{%
%  \popQED\endtrivlist\@endpefalse
%  \addvspace{20pt} % some space after
%}
%
%\renewcommand{\qedhere}{\hfill \qedsymbol}
%\makeatother





% -------------------------- POCZĄTEK --------------------------


% --------------------- Ustawienia użytkownika ------------------

\newcommand{\tytul}{Zastosowanie sieci neuronowych do identyfikacji nukleotydów z danych pochodzących z sekwencjonowania technologią Nanopore}
\renewcommand{\title}{Application of neural networks to identify nucleotides from data derived from Nanopore sequencing}
\newcommand{\type}{magisters} % magisters, licencjac
\newcommand{\supervisor}{dr hab. Dariusz Plewczyński}



\begin{document}
\sloppy

\includepdf[pages=-]{strona_tytulowa-jeden-autor.pdf}


% ------------------ STRONA Z PODPISAMI AUTORA/AUTORÓW I PROMOTORA ------------------


\thispagestyle{empty}\newpage
\null

\vfill

\begin{center}
\begin{tabular}[t]{ccc}

............................................. & \hspace*{100pt} & .............................................\\
podpis promotora & \hspace*{100pt} & podpis autora


\end{tabular}
\end{center}



% ---------------------------- ABSTRAKTY -----------------------------
% W PRACY PO POLSKU, NAPIERW STRESZCZENIE PL, POTEM ABSTRACT EN

{
\begin{abstract}

\begin{center}
\tytul
\end{center}

Sekwencjonowanie DNA przy użyciu nanoporów przez które przepływa nić DNA charakteryzuje się niskim kosztem i znaczną długością otrzymanych odczytów. Kluczowy wpływ na jakość wyniku tego procesu ma krok basecallingu czyli translacji sygnału elektrycznego na sekwencję nukleotydów. Celem pracy jest zbudowanie narzędzia realizującego ten krok.

W pracy zaproponowano rozwiązanie oparte o uczenie głębokie. Przetestowano architektury w pełni konwolucyjne i takie wykorzystujące warstwy rekurencyjne. Udało się osiągnąć
odczyty o stopniu identyczności 91.15\% i konsensus w 99.77\% zgodny z referencją. Dokonano analizy wyników w celu wykrycia cech fragmentów sygnału, w których pojawiają się błędy.\\


\noindent \textbf{Słowa kluczowe:} sekwencjonowanie DNA, uczenie głębokie, modelowanie sekwencji, sekwencjonowanie nanopore, sieci neuronowe
\end{abstract}
}

\null\thispagestyle{empty}\newpage

{\selectlanguage{english}
\begin{abstract}

\begin{center}
\title
\end{center}

DNA sequencing by translocation of DNA strand through a nanopore offers lower cost and produces long reads. The process of translation of electric signal to sequence of nucloetides, called basecalling, strongly influences the quality of the result of sequencing. In this work a tool for basecalling is presented.

A deep learning based solution is presented. Several neural network architectures were tested, including fully convolutional ones and ones using recurrent layers. Best one produces reads with identity rate up to 91.15\% and assembly identity rate up to 99.77\%. Results are analyzed to discover features of segments of input electrical signal  that cause errors.  \\

\noindent \textbf{Keywords:} DNA sequencing, deep learning, sequence modelling, nanopore sequencing, neural networks
\end{abstract}
}


% --------------------- OŚWIADCZENIE -----------------------------------------


\null\thispagestyle{empty}\newpage

\null \hfill Warszawa, dnia ..................\\

\par\vspace{5cm}

\begin{center}
Oświadczenie
\end{center}

\indent Oświadczam, że pracę \type ką pod
tytułem ,,\tytul '', której promotorem jest \supervisor , wykonałam/wykonałem
samodzielnie, co poświadczam własnoręcznym podpisem.
\vspace{2cm}


\begin{flushright}
  \begin{minipage}{50mm}
    \begin{center}
      ..............................................

    \end{center}
  \end{minipage}
\end{flushright}

\thispagestyle{empty}
\newpage

\null\thispagestyle{empty}\newpage


% ------------------- 4. Spis treści ---------------------
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}

\newpage % JEŻELI SPIS TREŚCI MA PARZYSTĄ LICZBĘ STRON, ZAKOMENTOWAĆ
% ALBO JAK KTOŚ WOLI WTEDY DWIE STRONY ODSTĘPU, DODAĆ \null\newpage
\null\newpage
% -------------- 5. ZASADNICZA CZĘŚĆ PRACY --------------------
\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{12} % JEŻELI Z POWODU DUŻEJ ILOŚCI STRON W SPISIE TREŚCI SIĘ NIE ZGADZA, TRZEBA ZMODYFIKOWAĆ RĘCZNIE

\chapter{Wstęp}
\markboth{}{Wstęp}
%\addcontentsline{toc}{chapter}{Wstęp}
Sekwencjonowanie DNA pozwala na poznanie genomu organizmu i wykorzystanie tej wiedzy np. w medycynie poprzez zbadanie wpływu kodu genetycznego na zachorowanie na nowotwory \cite{cancerNonCoding} czy choroby o podłożu genetycznym\cite{diagnosis}. Zastosowanie sekwencjonowania jest ograniczone przez jego koszt i długość fragmentów genomu będących jego wynikiem nazywanych odczytami. Oxford Nanopore MinION to technologia sekwencjonowania dostępna od 2015 r. ona pory przez, które przepływają pojedyncze nici DNA. Cechuje się ona niskim kosztem i dużą długością produkowanych odczytów. MinION mierzy sygnał elektryczny, którego poziom zmienia się w zależności od nukleotydów znajdujących się w porze  w danym momencie. Proces analizy mający na celu otrzymanie sekwencji DNA odpowiadającej nazywany jest basecallingiem i ma on kluczowy wpływ na liczbę błędów  w otrzymanej sekwencji. Jest on utrudniony poprzez obecność szumu w otrzymywanym sygnale. Poprawienie tego procesu umożliwiłoby użycie sekwencjonowania nanoporowego w zastosowaniach, w których niezbędne są sekwencje o bardzo dużej trafności\cite{snp}. Ważną miarą oceny jakości basecallera jest nie tylko liczba błędów w pojedynczych odczytach, ale też stopień identyczności utworzonego z nich konsensusu względem genomu referencyjnego.

Istniejące basecallery wykorzystują uczenie głębokie w celu translacji sygnału na sekwencję DNA. Problem modelowania sekwencji pojawia się w wielu zastosowaniach sieci neuronowych np. tłumaczeniu maszynowym czy syntezie mowy. Architektury sieci wykorzystanych do tego celu mogą być wykorzystane w procesie basecallingu.

Dzięki analizie błędów popełnianych przez basecaller można by wykryć przypadki, w których dokonanie poprawnej predykcji na podstawie sygnału dla tego modelu. Umożliwiłoby zaproponowanie metody, która pomoże w rozwiązaniu tych trudnych przypadków i umożliwi zbudowanie lepszego konsensusu.

W pracy zaproponowano modele oparte o modele rekurencyjne i w pełni konwolucyjne. Przetestowano Zmierzono jakość produkowanych odczytów i otrzymanych z nich konsensusu. Otrzymano odczyty o stopniu identyczności 91.15\% i konsensus w 99.77\% zgodny z referencją. Przeprowadzono również analizę błędów w konsensusie i wykryto przypadek odpowiedzialny za 78\% występujących w nim błędów. Opisano jak sposób mierzenia sygnału przez urządzenia powoduje pojawianie się tego błędu.


\chapter{Opis problemu}

\section{DNA}

DNA to molekuła składająca się z dwóch zwiniętych wokół siebie nici tworzących podwójną helisę. DNA jest nośnikiem informacji genetycznej, przenoszącej instrukcje dotyczące rozwoju, funkcjonowania i reprodukcji wszystkich znanych organizmów żyjących i niektórych wirusów. Podstawowym składnikiem budującym DNA są nukleotydy. Każdy z nich zawiera jedną z czterech z zasad: adeninę, cytozynę, guaninę lub tyminę, często oznaczanych pierwszą literą nazwy (odpowiednio A, C, G lub T). Sekwencja tych baz w DNA definiuje kod genetyczny. Kompletny kod genetyczny organizmu jest nazywany genomem. Długości genomów znacznie różnią się pomiędzy gatunkami.


\begin{table}
\centering
\begin{tabular}{ll}
	Bakteria e. coli                                                                           & $\sim$4.6 miliona  \\
	\begin{tabular}[c]{@{}l@{}}Schizosaccharomyces pombe\\ (grzyb jednokomórkowy)\end{tabular} & $\sim$12.5 miliona \\
	\begin{tabular}[c]{@{}l@{}}Drosophila melanogaster\\ (muszka owocowa)\end{tabular}         & $\sim$170 milionów \\
	Oryza sativa (ryż)                                                                         & $\sim$470 milionów \\
	Canis familiaris (pies)                                                                    & $\sim$2.4 miliarda \\
	Homo sapiens (czlowiek)                                                                    & $\sim$2.9 miliarda
\end{tabular}
\caption{\label{}Liczba par zasad w genomach wybranych organizmów.}
\end{table}

\section{Sekwencjonowanie DNA}

Sekwencjonowanie DNA to proces rozpoznawania sekwencji DNA danego organizmu. Wczesne metody sekwencjonowania (np. metoda Sangera opracowana w 1977 r. \cite{sequencingTechnologies}) były zbyt kosztowne do powszechnych zastosowań, lecz metody drugiej i trzeciej generacji umożliwiły zbadanie DNA organizmów na szerszą skalę i wykorzystanie tej wiedzy w celach takich jak badanie odporności na nowotwory\cite{cancerImmunity} i przyczyn zachorowań na nie\cite{cancerNonCoding}, badanie historii adaptacji w procesie ewolucji człowieka \cite{adaptation}, medycyna spersonalizowana do kodu genetycznego człowieka \cite{personalizedMedicine}, badania prenatalne \cite{prenatal} i diagnostyka chorób o podłożu genetycznym\cite{diagnosis}.

Wynikiem sekwencjonowania jest fragment całego genomu. nazywanym odczytem(\textit{read}). Długość odczytów jest zależna od użytej technologii - sekwencjonowanie technologią Illumina produkuje odczyty o długości do 600 par zasad przy celności do 99.9\%, natomiast przy użyciu technologii PacBio odczyty osiągają długość nawet do 100 tysięcy par zasad przy celności 87\%. Odczyty są dopasowywane i nakładane na siebie tak aby otrzymać cały genom(po angielsku ten proces jest nazywany \textit{genome assembly}).  Pomimo większej ilości błędów w długich odczytach są one potrzebne do dopasowania regionów repetytywnych, czyli podsekwencji genomu występujących w nim wielokrotnie. Dla tego do składania genomu wykorzystuje się zarówno odczyty długie jak i krótkie.

\section{Oxford Nanopore MinION}

Oxford Nanopore MinION to technologia sekwencjonowania dostępna publicznie od maja 2015 r. produkująca długie odczyty, nawet do 2 milionów par zasad. Wykorzystuje ona pory przez, które przepływają pojedyncze nici DNA. Pory osadzone są w rezystentnej membranie polimerowej, przez którą przepływa prąd elektryczny. Nukleotyd znajdujący się w porze wpływa na jego opór elektryczny, tym samym powodując zmianę napięcia. Sygnał elektryczny jest mierzony i to on jest rezultatem pracy urządzenia. Analizując go jest możliwe uzyskanie sekwencji DNA nici, która przepłynęła przez por \cite{nanoporeHuman}. Proces ten nazywany jest basecallingiem. Na sygnał elektryczny wpływają pojedyncze molekuły znajdujące się wewnątrz poru co wprowadza wiele szumu. Dodatkowo w najwęższym punkcie poru znajduje się jednocześnie 4-5 nukleotydów i wszystkie z nich wpływają na opór elektryczny poru i tym samym na mierzony sygnał. Zatem basecalling jest złożonym problemem, który ma kluczowe znaczenie dla jakości sekwencjonowania nanoporowego.

Urządzenie MinION jest bardzo przenośne (ma rozmiar zszywacza) i jego koszt jest znacznie mniejszy niż konkurencyjnych urządzeń. Było ono wykorzystywane nawet w terenie, na przykład podczas badania ognisk choroby w odległych rejonach\cite{ebola}. Niski koszt urządzenia pozwala na zastosowanie go na szerszą skalę niż innych technologii sekwencjonowania.

\section{Istniejące rozwiązania}

Oxford Nanopore Technologies (producent MinIONa) oferuje kilka basecallerów. Są to oficjalny basecaller Albacore, Guppy wykorzystujący procesory graficzne i Scrappie określany przez producenta jako "demonstracja technologii". Aktualne wersje Albacore i Guppy wykorzystują sieci neuronowe. W przeszłości ONT oferowało również NanoNet będący pierwszym basecallerem dokonującym oredykcji przy użyciu sieci neuronowych, a także Metrichor działający w chmurze obliczeniowej oparty o łańcuchy Markowa. 

Opublikowano również następujące basecallery z otwartym kodem źródłowym pochodzące od społeczności akademickiej.

DeepNano\cite{deepNano} wykonuje proces basecallingu w dwóch krokach. W pierwszym sygnał jest dzielony na fragmenty odpowiadające k-merom. W drugim dwukierunkowa rekurencyjna sieć neuronowa wykorzystuje statystyki powstałych w pierwszym kroku fragmentów do przewidzenia k-meru odpowiadającemu każdemu fragmentowi. 

BasecRAWller\cite{basecrawler} wykorzystuje dwie rekurencyjne sieci neuronowe. Pierwsza z nich przewiduje dla każdego kroku czasowego w sygnale wejściowym prawdopodobieństwo tego, że jest on granicą pomiędzy segmentom sygnału odpowiadającego kolejnym nukleotydom. Druga mapuje w ten sposób powstałe segmenty na odpowiadające nim bazy.

Chiron\cite{chiron} tłumaczy sygnał elektryczny bezpośrednio na sekwencję DNA. Wykorzystuje on model oparty o sieci konwolucyjne, sieci rekurencyjne i dekodowanie CTC.

Jakość basecallingu można ocenić wykorzystując celność pojedynczego odczytu lub celność konsensusu utworzonego z wielu odczytów pochodzących z tego samego rejonu genomu. Konsensus powstaje poprzez nałożenie na siebie wielu odczytów z pasującymi do siebie fragmentami. Obie te metryki oblicza się poprzez mapowanie uzyskanej sekwencji na znaną sekwencji referencyjnej. Miary te nie muszą być one skorelowane - odczyty o dużej liczbie losowych błędów mogą stworzyć konsensus o bardzo dużej celności, ponieważ losowe błędy zostaną wyeliminowane poprzez nałożenie na siebie odczytów. Natomiast systematyczne błędy zostaną powielone w konsensusie\cite{wick}.

Literatura porównująca basecallery pokazuje, że średnia dokładność otrzymanych odczytów jest w przedziale 80-90\%, a celność konsensus otrzymanego z wielu odczytów waha się pomiędzy 99.37\% a 99.94\%\cite{wick}. Zatem w genomie bakterii e. coli o długości około 4.6 miliona baz sekwencjonowanego technologią nanopore wystąpiłoby co najmniej 4.6 tysiąca insercji, delecji i substytucji. Taka liczba substytucji znacznie utrudnia użycie sekwencjonowania nanopore w zastosowaniach gdzie nawet zamiana pojedynczej bazy w genomie, nazywana w skrócie SNP(single nucleotide polymorphism), ma kluczowe znaczenie\cite{snp}. Zatem stworzenie basecallera o lepszej precyzji zwiększyłoby zakres zastosowań technologii.


\begin{table}
	\centering
	\begin{tabular}{llll}
		Basecaller   & Mediana celności odczytu & Mediana celności konsensusu & \begin{tabular}[c]{@{}l@{}}Średnia długość odczytu \\ względem referencji\end{tabular} \\
		Albacore     & 88\%                     & 99.37\%                     & 99.8\%                                                                                 \\
		Guppy        & 88\%                     & 99.37\%                     & 99.8\%                                                                                 \\
		Scrappie     & 86\%                     & 99.42\%                     & 95\%                                                                                   \\
		Chiron       & 86\%                     & 99.4\%                      & 98\%                                                                                   \\
		DeepNano     & 78\%                     & 96\%                        & 99\%                                                                                   \\
		basecRAWller & 74\%                     & 85\%                        & 106\%                                                                                 
	\end{tabular}
	\caption{\label{}Porównanie jakości dostępnych Basecallerów. Wyniki pochodzą z basecallingu danych z sekwencjonowania \textit{K. pneumoniae}\cite{wick}}
\end{table}

\chapter{Opis rozwiązania}

\section{Przygotowanie zbiorów danych danych}

W celu nauczenia sieci neuronowej przygotowane zostały 2 zbiory danych w postaci par \textit{wektor sygnału, odpowiadające mu nukleotydy}.
Pierwszy zbiór powstał z danych z sekwencjonowania genomu ludzkiego z linii komórkowej NA12878. Wykorzystano DNA pochodzące z chromosomu mitochondrialnego, na które składa się 5000 odczytów.
Dane pochodzące z sekwencjonowania metodą Nanopore pochodzą z repozytorium udostępnionego przez Whole Human Genome Sequencing Project\cite{nanoporeHuman}, a genom referencyjny sekwencjonowany metodą Illumina pochodzi z The International Genome Sample Resource\cite{refGenome}. Aby uzyskać dane treningowe nie będące obciążone błędem innego \textit{basecallera} wykorzystano pakiet \textit{nanoraw}\cite{nanoraw}. Dokonuje on uliniowienia sygnału pochodzącego z sekwencjonowania technologią nanopore i genomu referencyjnego. W efekcie otrzymano podział sygnału na fragmenty, z których każdy odpowiada jednemu z nukleotydów.

W celu poprawienia generalności rozwiązania do uczenia sieci wykorzystano również zbiór danych z sewkwencjonowania gatunków \textit{Escherichia virus Lambda} i \textit{Escherichia coli} składający się z 4000 odczytów \cite{chironData}. Uliniowienie z genomami referencyjnymi zostało wykonane przez autorów zbioru, w sposób analogiczny do opisanego powyżej.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{squiggle}
	\caption{Wizualizacja dopasowania baz do znormalizowanego sygnału}
\end{figure}

\section*{Eksploracyjna analiza danych}

Sygnał elektryczny o całkowitej długości 246 892 693 próbek został dopasowany do sekwencji składających się w sumie z 29 193 641 par zasad. Zatem długość segmentu odpowiadającego jednej bazie wynosi 8.457071. Długość takich segmentów ma kluczowe znaczenie w budowie modelu, ponieważ na poziom sygnału w danym kroku czasowym ma wpływ od 4 do 5 baz obecnie znajdujących się w porze. Zatem, aby wykonać poprawną predykcję model potrzebuje informacji o bazach w odległości 2 baz od tej, dla której dokonywana jest predykcja. Średnia długość fragmentu sygnału odpowiadającego 5-merowi wyniosła 33.81744, z odchyleniem standardowym równym 21.70463. Tak duże odchylenie standardowe wynika z obecności małej liczby 5-merów o sygnale bardzo dużej długości - 99\% fragmentów sygnału odpowiadających 5-merowi miało długość mniejszą niż 79, 99.95\% mniejszą niż 209, a 99.9\% niż 620. Najdłuższy taki fragment sygnału miał długość 10197. Podczas sekwencjonowania nić dna przepływa przez por z prędkością 250-450 baz na sekundę a sygnał jest próbkowany 4000 razy na sekundę. Zatem na jeden 5-mer powinno przypadać do 80 próbek sygnału. Większa ich liczba może świadczyć o zablokowaniu się nici w porze lub innej anomalii w procesie sekwencjonowania. Ponad 99\% 5-merów w danych treningowych spełnia ten warunek. Długie fragmenty sygnału odpowiadające jednej bazie zostały odrzucone i nie były wykorzystywane w procesie uczenia sieci.

\begin{table}[]
	\centering
	\begin{tabular}{ll}
		10\%    & 21  \\
		30\%    & 26  \\
		50\%    & 31  \\
		70\%    & 37  \\
		90\%    & 49  \\
		99\%    & 79  \\
		99.5\%  & 94  \\
		99.9\%  & 155 \\
		99.99\% & 620
	\end{tabular}
	\caption{\label{}Wybrane kwantyle długośći syngałów odpowiadających 5-merow nukleotydów.}
\end{table}

\section{Ogólna architektura rozwiązania}

Problem basecallingu polega na translacji pewnej sekwencji wejściowej na sekwencję wyjściową o zmiennej ale mniejszej długości. Sekwencje wejściowe pochodzą z innej domeny niż sekwencje wyjściowe. Problem ten posiada wiele podobieństw do problemów takich jak rozpoznawanie mowy, rozpoznawanie pisma pisanego, segmentacja obrazu czy tłumaczenie maszynowe, do których z sukcesami zastosowano sieci neuronowe i uczenie głębokie. Modele i techniki użyte do tych zastosowań mogą być skuteczne w dziedzinie basecallingu.

Z racji na dużą długość odczytów sygnał odpowiadający całemu odczytowi musi zostać podzielony na mniejsze fragmenty, które mogą zostać przetworzone przez sieć. Każdy z tych segmentów powinien być wystarczająco długi, aby odpowiadać kilku 5-merom nukleotydów. Z sygnału powinny zostać wyekstrahowane pewne cechy lokalne względem kroku czasowego. Sieć powinna zawierać również moduł, który znajdzie zależności pomiędzy tymi cechami, aby dokonać segmentacji i dopasować sygnał do baz. Moduł ten powinien wykorzystywać cechy dla wszystkich kroków czasowych, wchodzących w skład 5-meru do którego należy obecny krok czasowy. Na podstawie wyniku obliczeń tego modułu zostanie obliczone prawdopodobieństwo, że próbka sygnału z kroku czasowego odpowiada jednej z baz lub granicy pomiędzy nimi. Z racji tego, że długość sekwencji wyjściowej jest zmienna należy wykorzystać dekodowanie CTC\cite{ctc}. Wyniki obliczone dla każdego krótkiego fragmentu muszą zostać złożone w konsensus reprezentujący sekwencję DNA dla całego odczytu.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{architecture}
	\caption{Schemat wysokopoziomowej architektury rozwiązania.}
\end{figure}


\section{Reprezentacja i przygotowanie danych wejściowych}

Sygnał jest normalizowany poprzez odjęcie średniej dla odczytu i podzielenie przez odchylenie standardowe. Sygnał pochodzący z jednego odczytu. jest dzielony na fragmenty po 300 próbek. Zgodnie z przedstawionymi wcześniej kwantylami długości fragmentów sygnału odpowiadającym 5-merom podsekwencje o takiej długości w ponad 99.9\% przypadków odpowiadają co najmniej kilku 5-merom. Zastosowanie takiej długości umożliwia również uczenie sieci w batchach o wielkości ponad 100 przykładów co przyspiesza proces uczenia. 

\section{Wykorzystane koncepty z dziedziny uczenia głębokiego}

W kolejnych sekcjach opisano techniki uczenia głębokiego i architektury sieci, które wykorzystano w testowanych modelach.

\subsection{ReLU}

Funkcje aktywacji są stosowane w sieciach neuronowych w celu dodania nieliniowości do modelu. Są one stosowane do wyniku obliczeń warstw wewnątrz sieci.We wszystkich modelach wykorzystano funkcję aktywacjj \textit{Recitified linear unit} lub w skrócie ReLU daną wzorem
\[Relu(x) = max(0,x)\]

\subsection{\textit{Batch Normalization}}

\textit{Batch normalization} to metoda przyspieszająca uczenie sieci neuronowych, polegająca na normalizacji danych wejściowych do warstw ukrytych. Dzięki temu rozkład wartości aktywacji tych warstw pozostaje taki sam podczas całego procesu uczenia sieci. Wynik zastosowania \textit{Batch normalization } dla danych wejściowy $h$ jest danym wzorem:
\[BN(h, \lambda, \beta)=\beta+\lambda\frac{h-\hat{E}}{\sqrt{\hat{Var}(h)+\epsilon}}\]
gdzie $\lambda$ i $\beta$ są wagami nauczonymi się przez sieć a $\epsilon$ to parametr regularyzujący.

\subsection{Sieci konwolucyjne, ResNet}

Sieci konwolucyjne(splotowe) to klasa głębokich sieci neuronowych wykorzystujących operację konwolucji. Charakteryzują się one mniejszą ilością parametrów i lepszą umiejętnością uczenia się lokalnych cech danych wejściowych niż sieci w pełni połączone\cite{cnn} i dzięki temu znajdują zastosowanie w ekstrakcji cech w problemach, w których dane wejściowe są zależne od położenia lub czasu, takich jak rozpoznawanie obrazu \cite{vgg}, wideo\cite{cnnVideo} lub dźwięku\cite{cnnAudio}. 

W przypadku sieci konwolucyjnych składających się z wielu warstw występuje problem zanikającego gradientu, co znacznie spowalnia lub uniemożliwia uczenia takich sieci\cite{difficulty}.
Architekturą, która próbuje rozwiązać tą trudność jest \textit{resNet}, osiągający jedne z najlepszych wyników w dziedzinie klasyfikacji obrazu. Sieci tego typu składają się z bloków zawierających połączenia omijające część warstw. Na wyjściu z każdego bloku znajduje się suma danych wejściowych i danych przetworzonych przez warstwy wewnątrz bloku. Dzięki temu sygnał nie zanika przed osiągnięciem dalszej części sieci. Umożliwiło to trenowanie sieci o większej liczbie warstw bez utraty celności predykcji.\cite{resnet}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{resnet}
	\caption{Schemat połączeń w sieci ResNet składającej się z 3 bloków.}
\end{figure}

Ulepszona wersja tej architektury zawierająca mapowania identycznościowe została zaprezentowana w \cite{preResnet}. Autorzy proponują taką modyfikację sieci, aby dane wejściowe były dodawane do kolejnych warstw bez żadnych modyfikacji. We wcześniejszej wersji do danych wejściowych stosowano funkcje aktywacji i funkcje normalizujące. Zapobiega to zanikaniu gradientu nawet w sieciach składających się z tysiąca warstw. Dzięki temu można uniknąć efektu pogarszania się jakości predykcji wraz ze zwiększaniem ilości warstw w sieci powyżej określonej liczby.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{resnetBlock}
	\caption{Konstrukcja bloku użytego w sieci resNet w wersji a) oryginalnej i b) z mapowaniem identycznościowym}
	\label{fig:resnetBlock}
\end{figure}

\subsection{DenseNet}

W architekturze DenseNet, będącej rozwinięciem idei wykorzystanych w ResNet, każda z warstw przekazuje do dalszej części sieci skonkatenowane wyniki obliczeń wykonanych przez siebie i każdą z poprzednich warstw. Na wyjściu z sieci znajdują się cechy obliczone, przez każdą z warstw w sieci - wynikami kolejnych warstw są cechy danych wejściowych coraz wyższego rzędu. Dzięki temu wszystkie warstwy w sieci podlegają nadzorowi, podobnie jak w sieciach DSN(Deeply-Supervised Nets)\cite{DSN}. Wykorzystanie cech obliczonych w początkowych warstwach w dalszej części sieci, umożliwia zastosowanie modeli o mniejszej liczbie parametrów. Klasyfikacja jest dokonywana na podstawie cech niskiego i wysokiego rzędu. Dzięki temu sieci typu DenseNet osiągają lepszą precyzję klasyfikacji wielu problemach niż sieci typu ResNet, mimo zastosowania mniej złożonego modelu\cite{denseNet}. 

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.6]{densenet}
	\caption{Schemat połączeń w sieci DenseNet składającej się z 3 bloków.}
	\label{fig:denseNet}
\end{figure}

\subsection{Sieci rekurencyjne, LSTM}

Sieci rekurencyjne umożliwiają przetwarzanie danych zależnych od czasu, wykorzystując do obliczenia wyniku dla pewnego kroku czasowego informacji obecnej w poprzednich krokach. Składają się one z komórek, jednej dla kroku czasowego, z których każda przyjmuje na wejściu stan obliczony przez poprzednią komórkę i dane wejściowe dla kroku czasowego,  a zwraca wynik i oblicza stan, który jest przekazywany do następnej komórki. Ponieważ wagi są dzielone pomiędzy krokami czasowymi sieci rekurencyjne można interpretować jako wykonanie obliczeń w pętli przez sieć składającą się z jednej takiej komórki. Dzięki temu że stan jest przekazywany pomiędzy kolejnymi krokami sieci rekurencyjne mają mechanizm pamięci i mogą znajdować zależności pomiędzy danymi oddalonymi od siebie o wiele kroków czasowych. \cite{rnn}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{rnn}
	\caption{Sieć rekurencyjna w postaci rozwiniętej i w postaci pętli}
\end{figure}

W praktyce sieciom rekurencyjnym nie udawało się nauczyć się zależności pomiędzy danymi oddalonymi od siebie o więcej niż kilka kroków czasowych \cite{rnnDifficulty}. Rozwiązaniem tego problemu okazały się sieci LSTM(Long short-term memory) wykorzystujące specjalną konstrukcję komórki \cite{lstm}. Składa się ona z czterech warstw i przekazuje do kolejnej komórki jako stan dwie zmienne $h_t$ i $C_t$ opowiadające pamięci krótko i długotrwałej.  

Komórka LSTM dla kroku czasowego $t$ wykonuje następujące operacje:
\begin{itemize}
	\item Na podstawie stanu $h_{t-1}$ i danych wejściowych $x_t$ obliczany jest wektor $f_t$ o wartościach z przedziału od 0 do 1, który reprezentuje, które wartości ze stanu $C_{t-1}$ powinny zostać zapomniane. 
	\item Wektor $g_t$ powstaje na podstawie $h_{t-1}$ i $x_t$. Reprezentuje on zmienne, które powinny zostać dodane do pamięci długotrwałej.
	\item Stan $C_t$ jest obliczony przez pomnożenie wektora $C_{t-1}$ przez $f_t$ a następnie dodanie $g_t$
	\item Na podstawie wektorów stanu $C_t$ i $h_{t-1}$ i danych wejściowych $x_t$ zostaje obliczony wektor $h_t$ będący jednocześnie wynikiem dla kroku czasowego $t$ i jednym z dwóch stanów~przekazanych do następnej komórki
\end{itemize}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.6]{lstm}
	\caption{Konstrukcja komórki sieci LSTM}
\end{figure}

Sieci LSTM są wykorzystywane w problemach takich jak rozpoznawanie mowy\cite{lstmSpeech}, analiza wideo\cite{lstmVideo}, rozpoznawanie pisma \cite{lstmHandwriting} i tłumaczenie maszynowe\cite{lstmTranslation}.

\subsection{Sieci w pełni konwolucyjne}

Sieci w pełni konwolucyjne to rodzaj sieci składających się wyłącznie z warstw konwolucyjnych, które mogą być zastosowane w problemach w których rozmiar danych wyjściowych jest zależny od rozmiaru danych wejściowych takich jak segmentacja obrazu\cite{segmentation} czy modelowanie sekwencji np. dzwięku i tekstu \cite{sequenceModelling}. Wykorzystują one fakt, że każdy neuron w sieci przetwarza tylko dane wejściowe znajdującym się w w jego otoczeniu, którego rozmiar zależy od rozmiaru filtrów w poprzedzających go warstwach. Zatem w sieci składające się wyłącznie z warstw konwolucyjnych, liczba neuronów w ostatniej warstwie jest zależna od rozmiaru danych wejśćiowych i każdy z tych neuronów odpowiada, pewnemu elementowi z danych wejściowych, np. krokowi czasowemu lub pikselowi w obrazie\cite{fcn}.

Czasowe sieci konwolucyjne(\textit{temporal convolutional networks} lub TCN) to sieci w pełni konwolucyjne dostosowane do problemów, w których dane wejściowe zależą od czasu. Stosuje się w nich konwolucje przyczynowe(causal convolutions), czyli takie, w których predykcja dla kroku czasowego \textit{t} zależy tylko od danych wejściowych dla kroku czasowego \textit{t} i wcześniejszych. Aby zwiększyć pole recepcyjne wykorzystano konwolucje rozszerzone (\textit{dilated convolutions}). Polega ona na wykorzystaniu wartości oddalonych o \textit{d} kroków czasowych przy obliczaniu wartości filtra, gdzie \textit{d} jest parametrem. Paramter ten jest zwiększany wykładniczo w kolejnych warstwach, co pozwala na wykorzystanie w predykcji danych nawet z kilkuset kroków czasowych wstecz.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{tcn}
	\caption{Schemat przedstawiający połączenia w sieci używającej konwolucji czasowych i przyczynowych.}
\end{figure}

Czasowe sieci konwolucyjne znalazły zastosowanie w problemach, w których dane wejściowe są zależne od czasu np. tłumaczenie maszynowe\cite{fcnTranslation} i synteza dźwięku. Innym typem sieci znajdujących zastosowanie w tej klasie problemów są sieci LSTM\cite{lstm}, lecz zademonstrowano, że sieci w pełni konwolucyjne są w stanie osiągać lepsze wyniki w wielu problemach, mimo mniejszego zużycia pamięci i krótszego czasu potrzebnego do wykonania obliczeń \cite{sequenceModelling}.

\subsection{WaveNet}

Wavenet to głęboka sieć neuronowa wykorzystana oryginalnie do syntezy mowy\cite{wavenet}. Wykorzystano w niej konwolucje rozszerzone i przyczynowe tak jak w czasowych sieciach konwolucyjnych. Do wyniku obliczeń każdej z warstw konwolucyjnych stosuje się funkcję sigmoidalną i tangens hiperboliczny. Do dalszej części sieci przekazywany iloczyn wyników zastosowania tych funkcji. Podobna operacja jest wykonywana w sieciach LSTM \cite{lstm}, i tak ja tam ma za zadanie znormalizowanie cech i wybranie, które z nich mają przekazane do dalszej części sieci. Nazywana ona jest Gated Linear Unit\cite{glu}. Dodatkowo wykorzystano połączenia rezydualne podobne do tych w sieci ResNet\cite{resnet}, a klasyfikacja jest dokonywana na podstawie sumy cech obliczonej przez każdą z warstw.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{wavenet_block}
	\caption{Blok sieci Wavenet. Sieć składa się z pewnej liczby takich bloków połączonych w serii.}
	\label{fig:wavenet_block}
\end{figure}

Sieci Wavenet udało się wygenerować mowę bardzo podobną do ludzkiej\cite{wavenet}, znalazła ona również zastosowanie w problemach rozpoznawania mowy, redukcji hałasu i generowania muzyki\cite{wavenetReview}.

\subsection{Dekodowanie CTC}

Zarówno sieci konwolucyjne jak i sieci LSTM mogą przyjmować dane wejściowe różnych rozmiarów, ale rozmiar danych wyjściowych jest funkcją rozmiaru danych wejściowych. W celu zbudowania modelu, który dla danych o określonym rozmiarze może zwracać sekwencje różnych długości stosuje się dekodowanie CTC(\textit{Connectionist Temporal Classification})\cite{ctc}. Dekoder CTC przyjmuje na wejściu macierz o rozmiarze \textit{(rozmiar alfabetu wyjściowego + 1, długość sekwencji wejściowej)}. Element macierzy na pozycji  \textit{(n, t)} opisuje prawdopodobieństwo, że krok czasowy \textit{t} danych wejściowych odpowiada \textit{n}-temu elementowi alfabetu wyjściowego. Ostatni wiersz macierzy odpowiada prawdopodobieństwu wystąpienia dodatkowego symbolu - znaku pustego odpowiadającemu granicom pomiędzy kolejnymi znakami w ciągu wyjściowym. 

Sekwencje są kodowane w tej macierzy przez ścieżki, które powstają poprzez wybranie jednego znaku dla każdego kroku czasowego. Ścieżka zostaje zamieniona na sekwencję wyjściową poprzez połączenie występujących w niej nieprzerwanych podciągów składających się z jednego znaku w pojedynczy symbol(AAAAAA -> A), a następnie usunięcie znaków pustych. Zatem sekwencje wyjściowe mogą być krótsze niż sekwencje wejściowe i mogą zawierać te same znaki na sąsiadujących pozycjach, w przypadku gdy w ścieżce znajduje się pomiędzy nimi znak pusty. Prawdopodobieństwo ścieżki jest zdefiniowane przez iloczyn prawdopodobieństw znaków wybranych dla każdego kroku czasowego.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.8]{ctc_array}
	\caption{Przykład dekodowania CTC dla alfabetu wyjściowego {A,C,G,T} i sekwencji wejściowej długości 6.}
\end{figure}

Prawdopodobieństwo sekwencji wyjściowej to suma prawdopodobieństw wszystkich ścieżek, które mogą zostać zamienione na tą sekwencję. Znalezienie optymalnego rozwiązania problemu wybrania ścieżki o największym prawdopodobieństwie jest zbyt złożone obliczeniowo, dlatego zamiast tego stosuje się algorytm zachłanny, który wybiera dla każdego kroku czasowego znak o największym prawdopodobieństwie lub algorytm \textit{beam search}, który dla każdego kolejnego kroku czasowego zapamiętuje określoną liczbę najlepszych kandydatów. W przypadku dokonywania predykcji przez model jako sekwencja wyjściowa wybierana jest ta znaleziona przez jeden z tych algorytmów.

Funkcja straty dla dekodowania CTC zdefiniowana jest jako
\[-\ln{(p(x|z))}\]
gdzie $p(x|z)$ to prawdopodobieństwo prawdziwej sekwencji dla danej macierzy prawdopodobieństw dla kroków czasowych. Jest ona różniczkowalna przy pomocy sieci neuronowych i algorytmu spadku gradientu.


\section{Architektury przetestowanych sieci neuronowych}

W trakcie tworzenia pracy przetestowano poniższe architektury sieci neuronowych.

\subsection{ResNet + LSTM z połączeniami rezydualnymi}

Na dane wejściowe nakładana jest warstwa konwolucyjna zmieniająca ich kształt tak, aby ostatni wymiar miał rozmiar 256. Jest to potrzebne, aby mogły one zostać wykorzystane w połączeniach rezydualnych. Następnie w sieci znajduje się 5 bloków rezydualnych z mapowaniem identycznościowym z warstwami konwolucyjnymi obliczającymi 256 cech. Konstrukcja takiego bloku została przedstawiona na rysunku \ref{fig:resnetBlock}. Po nich następuje 3 warstwowa sieć LSTM obliczająca 128 cech. Jest ona aplikowana do cech obliczonych przez poprzednie warstwy w obu kierunkach względem wymiaru czasu - od początku od końca i od końca do początku. Zatem każda z warstw oblicza również po 256 cech. Również w tej części sieci zostały dodane połączenia rezydualne omijające warstwę. Są one sumowane z cechami obliczonymi przez warstwę, które omijają. Zatem sygnał wejściowy może płynąć przez całą sieć bez żadnych modyfikacji poza zmianą kształtu w pierwszej warstwie. Ostatnią warstwą w sieci jest warstwa gęsto połączona która dla każdego kroku czasowego oblicza prawdopodobieństwo że odpowiada on któremuś z nukleotydów lub granicy pomiędzy nimi. Wynik tej warstwy trafia do dekodera CTC, który zwraca sekwencję DNA.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{resnetLstm}
	\caption{Architektura modelu ResNet + LSTM z połączeniami rezydualnymi}
\end{figure}

\subsection{Deep ResNet + LSTM z połączeniami rezydualnymi}

Model, tak jak poprzedni, składa się z bloków ResNet i dwukierunkowych warstw LSTM. Zwiększono jednak głębokość sieci - zastosowano 10 bloków ResNet i 5 warstw LSTM. Dzięki zastosowaniu połączeń rezydualnych zwiększenie złożoności modelu powinno poprawić jakość predykcji.

\subsection{ResNet + Konwolucje czasowe}

Podobnie jak w poprzedniej sieci do ekstrakcji cech lokalnych wykorzystano 5 bloków ResNet. Po nich następują bloki wykorzystujące warstwy konwolucyjne rozszerzone i przyczynowe. W każdym bloku znajdują się 4 warstwy. Najpierw aplikowane są dwie z nich - jedna w oryginalnym kierunku a druga w odwróconym. Efekt zastosowania warstwy w odwróconym kierunku jest uzyskany poprzez odwróceniu danych wejściowych względem wymiaru odpowiadającemu czasowi, zastosowanie warstwy i odwróceniu otrzymanego wyniku. Oba te wyniki są konkatenowane, a następnie stosuje się do nich \textit{Batch normalization} i funkcję aktywacyjną ReLu Dwie kolejne warstwy wykonują tą samą operację. Każda z warstw oblicza po 128 cech. Wszystkie warstwy wewnątrz danego bloku mają tą samą wartość rozszerzenia(dilation), lecz w każdym kolejnym bloku jest ona podwajana. W sumie w sieci znajduje się 7 takich bloków, czyli ostatni ma wartość rozszerzenia równą 64. Jądro ma wielkość 3, zatem pole receptywne konwolucji w ostatniej warstwie sięga 128 kroków czasowych w obu kierunkach. Warstwa gęsto połączona jest aplikowana do sumy wyników obliczeń każdego z bloków.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{tcn_block}
	\caption{Budowa bloku wykorzystującego konwolucje czasowe}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{resnetTcn}
	\caption{Architektura modelu ResNet + Konwolucje czasowe}
\end{figure}

\subsection{ResNet + WaveNet}

Ten model jest rozszerzeniem poprzedniego o idee wykorzystane w WaveNet\cite{wavenet}. Do ekstrakcji cech lokalnych wykorzystano 5 bloków ResNet. Po nich następują bloki zawierające 2 warstwy konwolucyjne rozszerzone i przyczynowe. Do wyniku z nich jednej z nich aplikowana jest funkcja sigmoidalna a do drugiej tangens hiperboliczny, a następnie obliczany jest iloczyn. W ten sposób sieć może nauczyć się zapamiętywać lub usuwać z pamięci pewne cechy. Budowa bloku wykorzystanego w tej sieci jest podobna do tego przedstawionego na rysunku \ref{fig:wavenet_block}, z tą różnicą że warstwy konwolucyjne są aplikowane do danych oryginalnych i takich z odwróconym wymiarem czasu. Wyniki te są następnie konkatenowane. Jądro w tych warstwach ma rozmiar 2, a rozszerzenia przyjmują wartości od 1 do 128 zatem pole receptywne ma tą samą wielkość co w poprzednim modelu. Tak samo jak w poprzednim modelu dane wyjściowego każdego z bloków są sumowane i przekazywane do warstwy gęsto połączonej i następnie do dekodera CTC.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{resnetWavenet}
	\caption{Architektura modelu ResNet + WaveNet}
\end{figure}

\subsection{DenseNet + WaveNet}

W tym modelu do ekstrakcji cech lokalnych wykorzystano model wzorowany na sieci DenseNet\cite{denseNet}. Architektura tej została przedstawiona na rysunku \ref{fig:denseNet}. Składa się on z 30 warstw konwolucyjnych z \textit{batch normalization} i funkcją aktywacyjną ReLU. Każda z warstw oblicza 12 cech i konkatenuje je z danymi wejściowymi, które otrzymała. Po 30 warstwach zostaje zatem obliczone 360 cech. Po nich następuje warstwa która zmniejsza liczbę cech do 256 poprzez obliczenie kombinacji cech otrzymanych na wejściu. Dalsza część sieci jest analogiczna do poprzedniego modelu
. 
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{denseWaveNet}
	\caption{Architektura modelu DenseNet + WaveNet}
\end{figure}

\section{Proces uczenia}

Do uczenia sieci wykorzystano algorytm optymalizacyjny Adam\cite{adam}, z \textit{learning rate} równym 0.001. Optymalizowano funkcję straty CTC\cite{ctc}. Sieć była uczona przez maksymalnie 10 epok - proces uczenia był przerywany jeśli po 2 kolejnych epok wartość funkcji straty nie zmniejszała się. Do uczenia wykorzystano procesor graficzny GEFORCE GTX 1080.

\section{Składanie odczytów}

Podczas dokonywania predykcji długi odczyt sygnału dzielony jest na częściowo pokrywające się ze sobą fragmenty od długości 300 próbek. Każdy taki fragment pokrywa się z następnym w 90\%, to znaczy fragmenty zaczynają się od próbek sygnału o indeksach 0, 30, 60, 90 itd. Uzyskuje się w ten sposób dziesięciokrotne pokrycie krótkimi odczytami. Z racji że kolejność krótkich odczytów w konsensusie jest znana utworzenie go jest łatwe. Uzyskuje się go przez znalezienie najdłuższych zachodzących na siebie fragmentów sąsiadujących odczytów i nałożenie ich na siebie. W ten sposób zliczane są wystąpienia nukleotydów na każdej pozycji i wybierane są te, które pojawiają się najczęściej.

\begin{figure}[h!]
	\centering
	\includegraphics{assembly}
	\caption{Wizualizacja składanie krótkich odczytów w długi.}
\end{figure}

\section{Szczegóły techniczne implementacji}

Modele sieci neuronowych zostały zaimplementowane w języku Python, przy użyciu biblioteki TensorFlow\cite{tensorflow} wersja r1.15. Aplikacja obsługuje dane wejściowe w formacie fast5, będącym modyfikacją formatu HDF5 opracowaną przez Oxford Nanopore Technologies do przechowywania danych z sekwencjonowania MinIONem. Obsługiwane są również odczyty w formacie tekstowym wykorzystanym w jednym z użytych zbiorów danych\cite{chironData}. Sekwencje będące wynikiem basecallingu zapisywane są w formacie FASTA. Jest to format tekstowy służący do reprezentowania sekwencji nukleotydów lub białek, szeroko stosowany w bioinformatyce.

\chapter{Ewaluacja modeli i analiza wyników}

\section{Zbiory danych}

Do oceny jakości modeli wykorzystano zbiory danych zawierające po 2000 odczytów. Pochodziły one z sekwencjonowania próbek \textit{e. coli} i \textit{lambda phage}\cite{chironData}.

\section{Wykorzystane metryki}

Aby ocenić wynik basecallingu na krótkich fragmentach sygnału (300 próbek) obliczono liczbę insercji, substytucji i delecji potrzebnych do przetransformowania wyniku dekodowania CTC do sekwencji DNA, która faktycznie odpowiada fragmentowi sygnału dla którego dokonano basecallingu. Obliczona wartość jest dzielona przez liczbę baz w oczekiwanej sekwencji w celu znormalizowania miary. Otrzymuje się w ten sposób znormalizowany dystans edycji pomiędzy wynikiem basecallingu a fragmentem prawdziwej sekwencji DNA. Miara ta ocenia jakość krótkich fragmentów DNA będących wynikiem bezpośrednio otrzymywanym z sieci neuronowej, przed krokiem składania odczytu.

Z fragmentów DNA otrzymywanych na wyjściu dekodera tworzony jest konsensus odpowiadający sekwencji DNA pewnego odczytu. Sekwencje te są mapowane na odpowiedni genom referencyjny przy użyciu programu minimap2\cite{minimap}. Otrzymane w ten sposób pliki BAM zostały przeanalizowane programem japsa\footnote{https://github.com/mdcao/japsa}, który oblicza liczbę delecji, substytucji i insercji pomiędzy pomiędzy sekwencją wejściowa i fragmentem genomu referencyjnego na, który została zmapowana. Procent delecji, insercji i substytucji obliczony jest przez iloraz liczby tych operacji i długości fragmentu genomu referencyjnego, na który została zmapowana sekwencja. Stopień identyczności pomiędzy sekwencjami jest zdefiniowany jako $1-procentDelecji-procentSubstytucji$, a procent błędów jako $procentInsercji + procentDelecji + procentSubstytucji$.

Aby sprawdzić jakość konsensusu powstałego z otrzymanych odczytów wykorzystano assembler Rebaler\footnote{https://github.com/rrwick/Rebaler}. Zbudowano przy jego pomocy kontigi z odczytów z sekwencjonowania \textit{lambda phage}. Kontigi zostały podzielone na fragmenty zawierające 10 tysięcy par zasad i zmapowane na genom referencyjny przy pomocy minimap2. Następnie obliczono średni stopień identyczności pomiędzy zmapowanymi sekwencjami i genomem referencyjnym. Obliczono również względną długość otrzymanego konsensusu względem genomu referencyjnego. Taka procedura sprawdzenia jakości konsensusu pojawia się w publikacjach porównujących basecallery\cite{wick}\cite{chiron}.

\section{Ewaluacja testowanych architektur sieci}

\subsection{Proces uczenia}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.8]{training_loss}
	\caption{Wartości funkcji straty podczas procesu uczenia poszczególnych modeli.}
	\label{fig:training}
\end{figure}

Na wykresie \ref{fig:training} przedstawiono wartości funkcji straty osiągane przez modele w trakcie procesu uczenia. W przypadku wszystkich architektur strata spada szybko podczas pierwszej epoki, a potem zmienia się już tylko nieznacznie. Przez cały proces uczenia najmniejszą wartość funkcji straty notują modele oparte o sieci rekurencyjne. 

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.8]{val_distance}
	\caption{Znormalizowany dystans edycji liczony na zbiorze treningowym końcu każdej epoki.}
	\label{fig:val_distance}
\end{figure}
	
Podobnie podczas uczenia zachowuje się dystans edycji (wykres \ref{fig:val_distance}. W przypadku modelu DenseNet + WaveNet zwiększa się on podczas ostatniej epoki, co może świadczyć o przetrenowaniu modelu.

\subsection{Dystans edycji krótkich fragmentów}

Podczas basecallingu zbioru testowego obliczono znormalizowany dystans edycji pomiędzy wynikiem predykcji na fragmentach sygnału po 300 próbek, a sekwencjom DNA odpowiadających temu sygnałowi. Wyniki przedstawiono w tabeli \ref{table:edit_distance}. Wyniki te są bardzo podobne do tych uzyskanych na zbiorze treningowym - najlepsze osiągane są przez modele wykorzystujące sieci rekurencyjne.

\begin{table}[h!]
	\centering
	\begin{tabular}{ll}
		Model            & \begin{tabular}[c]{@{}l@{}}Znormalizowany \\ \\ Dystans Edycji\end{tabular} \\
		ResNet+LSTM      & 0.1544                                                                      \\
		Deep ResNet+LSTM & 0.1566                                                                      \\
		ResNet+TCN       & 0.1853                                                                      \\
		ResNet+WaveNet   & 0.2040                                                                      \\
		DenseNet+WaveNet & 0.1879                                                                     
	\end{tabular}
	\caption{\label{}Znormalizowany dystans edycji dla krótkich fragmentów sygnału obliczony na zbiorze testowym.}
	\label{table:edit_distance}
\end{table}

\subsection{Ocena jakości odczytów}

Sekwencje otrzymane z basecallingu całych odczytów zostały zmapowane na genom referencyjny w celu zbadania ich jakości. Wyniki zostały przedstawione w tabelach \ref{table:lambda} i \ref{table:ecoli}. Wszystkie modele produkowały trafniejsze odczyty dla danych \textit{e. coli}. Jakość odczytów jest powiązana z jakością predykcji dla krótkich fragmentów - modele, których wyniki dla krótkich fragmentów osiągały niższy znormalizowany dystans edycji produkowały odczyty o wyższym stopniu identyczności i niższym procencie błędów. Zatem ponownie modele wykorzystujące rekurencyjne sieci neuronowe osiągały lepsze wyniki niż oparte wyłącznie o sieci konwolucyjne.

\begin{table}[]
		\centering
	\begin{tabular}{llllll}
		& \begin{tabular}[c]{@{}l@{}}Procent\\ delecji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent\\ insercji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent \\ substytucji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Stopień\\ identyczności\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent \\ błędów\end{tabular} \\
		ResNet+LSTM      & 6.30\%                                                    & 2.26\%                                                     & 4.44\%                                                         & 89.26\%                                                         & 13.00\%                                                   \\
		Deep ResNet+LSTM      & 5.55\%                                                    & 2.59\%                                                     & 4.25\%                                                         & 90.19\%                                                         & 11.44\%                                                   \\
		ResNet+TCN       & 7.02\%                                                    & 3.00\%                                                     & 6.41\%                                                         & 86.57\%                                                         & 16.43\%                                                   \\
		ResNet+WaveNet   & 9.18\%                                                    & 1.82\%                                                     & 5.74\%                                                         & 85.09\%                                                         & 16.74\%                                                   \\
		DenseNet+WaveNet & 7.40\%                                                    & 2.56\%                                                     & 5.78\%                                                         & 86.83\%                                                         & 15.74\%                                                  
	\end{tabular}
	\caption{\label{}Miary jakości odczytów otrzymanych z poszczególnych modeli dla sygnału z sekwencjonowania \textit{lambda phage}}
	\label{table:lambda}
\end{table}

\begin{table}[h!]
	\centering
	\begin{tabular}{llllll}
		& \begin{tabular}[c]{@{}l@{}}Procent\\ delecji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent\\ insercji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent \\ substytucji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Stopień\\ identyczności\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent \\ błędów\end{tabular} \\
		ResNet+LSTM      & 5.42\%                                                    & 2.20\%                                                     & 4.20\%                                                         & 90.39\%                                                         & 11.82\%                                                   \\
		Deep ResNet+LSTM      & 4.77\%                                                    & 2.51\%                                                     & 4.08\%                                                         & 91.15\%                                                         & 11.44\%                                                   \\
		ResNet+TCN       & 6.11\%                                                    & 2.94\%                                                     & 5.94\%                                                         & 87.95\%                                                         & 13.99\%                                                   \\
		ResNet+WaveNet   & 8.15\%                                                    & 1.85\%                                                     & 5.59\%                                                         & 86.26\%                                                         & 15.59\%                                                   \\
		DenseNet+WaveNet & 6.57\%                                                    & 2.52\%                                                     & 5.67\%                                                         & 87.76\%                                                         & 13.76\%                                                  
	\end{tabular}
	\caption{\label{}Miary jakości odczytów otrzymanych z poszczególnych modeli dla sygnału z sekwencjonowania \textit{e. coli}}
	\label{table:ecoli}
\end{table}

\subsection{Ocena jakości konsensusu}

W tabeli \ref{table:consensus} zaprezentowano jakośc konsensusów otrzymanych z odczytów. Wyniki dla każdego z modelu zawierają błędy, częstotliwość ich występowania jest powiązana z iloscią błędów w odczytach, z których powstały kontigi. Zatem wszystkie modele popełniają systematyczne błędy, które zostają odzwierciedlone w konsensusie.

\begin{table}[h!]
	\begin{tabular}{lll}
		& \begin{tabular}[c]{@{}l@{}}Mediana stopnia\\ identycznośći\end{tabular} & \begin{tabular}[c]{@{}l@{}}Mediana długości\\ względem referencji\end{tabular} \\
		ResNet+LSTM      & 99.67\%                                                                 & 99.69\%                                                                        \\
		Deep ResNet+LSTM      & 99.77\%                                                                 & 99.80\%                                                                        \\
		ResNet+TCN       & 99.55\%                                                                 & 99.75\%                                                                        \\
		ResNet+WaveNet   & 98.94\%                                                                 & 99.02\%                                                                        \\
		DenseNet+WaveNet & 99.43\%                                                                 & 99.55\%                                                                       
	\end{tabular}
	\caption{\label{}Miary jakości konsensusu otrzymanych z poszczególnych modeli dla sygnału z sekwencjonowania \textit{lambda phage}}
	\label{table:consensus}
\end{table}

\subsection{Porównanie wyników, wnioski}

We wszystkich obliczonych metrykach lepsze wyniki osiągają architektury wykorzystujące warstwy rekurencyjne. Sieci wykorzystujące wyłącznie warstwy konwolucyjne produkują odczyty o większym dystansie edycji względem referencji i tym samym konsensus o mniejszym stopniu identyczności. Może to wynikać z faktu że w sieciach LSTM istnieje stan obliczeń przekazywany pomiędzy krokami czasowymi - dzięki temu predykcja dokonana w pewnym kroku czasowym wpływa na te dokonywane w innych krokach. 

Typ popełnianych błędów w odczytach jest podobny w prawie wszystkich modelach - pojawia się więcej delecji i substytucji niż insercji. Jedynie model ResNet+WaveNet cechuje się innym profilem błędów - występuję tam względnie więcej delecji a mniej insercji. Konsensus stworzony z dwóch zestawów odczytów o innym typie błędów może mieć lepszą jakość niż taki stworzony tylko jednego z nich, jednak  konsensusu z odczytów z różnych modeli nie poprawiało jego jakości. Zwiększenie liczby warstw w modelu ResNet+LSTM poprawiło jakość predykcji i to model DeepResNet+LSTM produkował odczyty o najwyższym stopniu identyczności i najmniejszej liczbie błędów. 

Jakość pojedynczych odczytów miała bezpośredni wpływ na utworzony z nich konsensus - mniejsza liczba błędów w odczytach przekładała się na mniejszą liczbę błędów w konsensusie. Najlepszy konsensus powstał z odczytów z modelu DeepResNet+LSTM, przeprowadzono dla niego analizę błędów.s

Prawie 80\% błędów występujących w konsensusie było delecją w homopolimerze czyli ciągu przynajmniej 3 tych samych baz sąsiadujących ze sobą. Sygnał pochodzący z takich fragmentów DNA może być trudny do przetworzenia przez sieć neuronową. Przykładowy fragment sygnału odpowiadający homopolimerowi składającemu się z ośmiu cząsteczek adeniny przedstawiono na wykresie \ref{homopolymer}. We fragmentach odpowiadających homopolimerom poziom sygnału zmienia się w małym stopniu, podczas gdy w innych fragmentach zmianie bazy odpowiada skok sygnału. Może to wynikać z faktu, że w przypadku homopolimerów dłuższych niż 5 baz przez dłuższy czas w porze znajduje się 5-mer, a zatem nie zmienia się opór. Dlatego model ma problemy z rozpoznaniem, że przez por przesunął się kolejny nukleotyd. Rozwiązaniem tego problemu może być osobny model, który wykryje fragmenty sygnału potencjalnie odpowiadające homopolimerom i dokona predykcji w inny sposób. Umożliwiłoby to wyeliminowanie znacznej części błędów w konsensusie.

\begin{table}[]
	\begin{tabular}{ll}
		Rodzaj błędu             & Procent wszystkich błędów \\
		Delecja w homopolimerze  & 78.81\%                   \\
		Insercja w homopolimerze & 0.84\%                    \\
		Inna insercja            & 11.01\%                   \\
		Inna delecja             & 4.23\%                    \\
		Substytucja              & 5.11\%                   
	\end{tabular}
	\caption{Rodzaj błedów popełnionych w konsensusie otrzymanym z readów basecallowanych modelem Deep ResNet+LSTM}
	\label{table:errors}
\end{table}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{homopolymer}
	\caption{Znormalizowany sygnał dla homopolimeru składającego się z ośmiu cząsteczek adeniny }
	\label{fig:homopolymer}
\end{figure}

\chapter{Podsumowanie}

Celem pracy było stworzenia narzędzia umożliwiającego dokonania translacji sygnału pochodzącego z sekwencjonowania nici DNA urządzeniem ONT MinION na sekwencję nukleotydów znajdującej się w niej. Wykorzystano do tego głębokie sieci neuronowe i przetestowano następujące ich architektury:

\begin{itemize}
	\item sieć opartą o bloki ResNet i warstwy LSTM
	\item głęboką sieć opartą o bloki ResNet i warstwy LSTM, wykorzystującą połączenia rezydualne
	\item sieć w pełni konwolucyjną wykorzystującą bloki ResNet i konwolucje czasowe
	\item sieć w pełni konwolucyjną oparta o bloki ResNet i konwolucje czasowe wykorzystujące Gated Linear Units
	\item sieć w pełni konwolucyjną oparta o sieć DenseNet i konwolucje czasowe wykorzystujące Gated Linear Units
\end{itemize}

Wszystkie modele wykorzystują dekodowanie CTC do obliczenia sekwencji wyjściowej. Zaimplementowano również algorytm umożliwiający stworzenie z krótkich sekwencji obliczanych przez sieć konsensusu reprezentującego sekwencję odpowiadającą długiemu odczytowi.

Zbadano jakość predykcji na wszystkich etapach obliczania wyniku:

\begin{itemize}
	\item obliczono znormalizowany dystans edycji dla krótkich sekwencji na wyjściu z sieci
	\item zmapowano otrzymane odczyty na genom referencyjny i obliczono liczbę delecji, insercji i substytucji
	\item zbudowano konsensus z odczytów i porównano otrzymane kontigi z  genomem referecyjnym
\end{itemize}

Według wszystkich miar najlepsze wyniki uzyskiwały modele wykorzystujące sieci rekurencyjne. Wykorzystanie modelu, który przekazuje stan obliczeń pomiędzy krokami poprawia jakość predykcji. Zastosowanie połączeń rezydualnych umożliwiło wytrenowanie głębszej sieci neuronowej, która osiągała lepsze wyniki, podobne do tych co istniejące rozwiązania.

Przeprowadzono analizę błędów w konsensusie utworzonych z odczytów pochodzących z modelu produkującego najtrafniejsze odczyty. Wykryto rodzaj błędu odpowiadający za 80\% wszystkich pomyłek w otrzymanym konsensusie - delecje w homopolimerach. Opisano jak sposób działania urządzenia MinION wpływa na występowanie tego błędu i dlaczego rozpoznanie liczby baz w homopolimerze jest trudnym problemem dla sieci neuronowej. W celu poprawy predykcji można stworzyć osobny model do wykrywania i dokonania predykcji regionów sygnału odpowiadających homopolimerom.

% -------------------- 6. Bibliografia -----------------------
% Bibliografia leksykograficznie wg nazwisk autorów
% Dla ambitnych - można skorzystać z BibTeX-a

\begin{thebibliography}{20}%jak ktoś ma więcej książek, to niech wpisze większą liczbę
% \bibitem[numerek]{referencja} Autor, \emph{Tytuł}, Wydawnictwo, rok, strony
% cytowanie: \cite{referencja1, referencja 2,...}
\bibitem{genome}James Fraser, Iain Williamson, Wendy A. Bickmore,  Josée Dostie, An Overview of Genome Organization and How We Got There: from FISH to Hi-C
\bibitem{cancerImmunity}Finotello, F., Rieder, D., Hackl, H. et al. Next-generation computational tools for interrogating cancer immunity. Nat Rev Genet 20, 724–746 (2019) doi:10.1038/s41576-019-0166-7
\bibitem{cancerNonCoding}Khurana, E., Fu, Y., Chakravarty, D. et al. Role of non-coding sequence variants in cancer. Nat Rev Genet 17, 93–108 (2016) doi:10.1038/nrg.2015.17
\bibitem{adaptation} Marciniak, S., Perry, G. Harnessing ancient genomes to study the history of human adaptation. Nat Rev Genet 18, 659–674 (2017) doi:10.1038/nrg.2017.65
\bibitem{personalizedMedicine} Ashley, E. Towards precision medicine. Nat Rev Genet 17, 507–522 (2016) doi:10.1038/nrg.2016.86
\bibitem{prenatal} Chiu, R. W. et al. Noninvasive prenatal diagnosis of fetal chromosomal aneuploidy by massively parallel genomic sequencing of DNA in maternal plasma. Proc. Natl Acad. Sci. USA105, 20458–20463 (2008)
\bibitem{diagnosis} Choi, M. et al. Genetic diagnosis by whole exome capture and massively parallel DNA sequencing. Proc. Natl Acad. Sci. USA106, 19096–19101 (2009).
\bibitem{sequencingComparison} Liu L, Li Y, Li S, Hu N, He Y, Pong R, Lin D, Lu L, Law M (1 January 2012). "Comparison of Next-Generation Sequencing Systems". Journal of Biomedicine and Biotechnology. 2012: 251364. doi:10.1155/2012/251364.
\bibitem{sequencing} Jay Shendure Shankar Balasubramanian, George M. Church, Walter Gilbert, Jane Rogers, Jeffery A. Schloss, Robert h. Waterston DNA sequencing at 40: past, present and future
\bibitem{sequencingTechnologies} James M.Heather, Benjamin Chain The sequence of sequencers: The history of sequencing DNA
\bibitem{refGenome} Susan Fairley, Ernesto Lowy-Gallego, Emily Perry, Paul Flicek, The International Genome Sample Resource (IGSR) collection of open human genomic variation resources, Nucleic Acids Research.
\bibitem{nanoporeHuman} Miten J. et al., Nanopore sequencing and assembly of a human genome with ultra-long reads, Nature Biotechnology 36, pp. 338–345 (2018) 
\bibitem{ebola} Hoenen T, Groseth A, Rosenke K, et al. Nanopore Sequencing as a Rapidly Deployable Ebola Outbreak Tool. Emerg Infect Dis. 2016;22(2):331–334. doi:10.3201/eid2202.151796
\bibitem{deepNano}Boza, V., Brejova, B. \& Vinar, T. Deepnano: Deep recurrent neural networks for base calling in minion nanopore reads.PloS one12, e0178751 (2017).
\bibitem{basecrawler}Stoiber, M. \& Brown, J. Basecrawller: Streaming nanopore basecalling directly from raw signal.bioRxiv133058 (2017).
\bibitem{chiron} Haotian Teng, Minh Duc Cao, Michael B Hall, Tania Duarte, Sheng Wang, Lachlan J M Coin, Chiron: translating nanopore raw signal directly into nucleotide sequence using deep learning, GigaScience, Volume 7, Issue 5, May 2018, giy037, https://doi.org/10.1093/gigascience/giy037
\bibitem{wick}Wick, R.R., Judd, L.M. \& Holt, K.E. Performance of neural network basecalling tools for Oxford Nanopore sequencing. Genome Biol 20, 129 (2019) doi:10.1186/s13059-019-1727-y
\bibitem{snp}Pightling AW, Pettengill JB, Luo Y, Baugher JD, Rand H, Strain E. Interpreting whole-genome sequence analyses of foodborne bacteria for regulatory applications and outbreak investigations. Front Microbiol. 2018; 9:1–13. https://doi.org/10.3389/fmicb.2018.01482.
\bibitem{nanoraw}Stoiber, M.H. et al. De novo Identification of DNA Modifications Enabled by Genome-Guided Nanopore Signal Processing. bioRxiv (2016).
\bibitem{chironData} Teng H; Cao MD; Hall MB; Duarte T; Wang S; Coin LJM (2018): Supporting data for "Chiron: Translating nanopore raw signal directly into nucleotide sequence using deep learning" GigaScience Database.
\bibitem{resnet} K. He, X. Zhang, S. Ren and J. Sun, “Deep Residual Learning for Image Recognition,” in CVPR, 2016.
\bibitem{preResnet}K. He, X. Zhang, S. Ren, and J. Sun. Identity Mappings in Deep Residual Networks. arXiv preprint arXiv:1603.05027v3,2016.
\bibitem{cnn}Yann LeCunn, Leon Bottou, Yoshua Bengio, Patrick Haffner, Gradient-Based Learning Applied to Document Recognition, 1998
\bibitem{vgg}Very Deep Convolutional Networks for Large-Scale Image Recognition Karen Simonyan, Andrew Zisserman
\bibitem{cnnVideo} Exploiting Image-trained CNN Architectures for Unconstrained Video Classification Shengxin Zha, Florian Luisier, Walter Andrews, Nitish Srivastava, Ruslan Salakhutdinov
\bibitem{cnnAudio} Environmental Sound Classification With Convolutional Neural Networks Karol J. Piczak
\bibitem{difficulty}Understanding the difficulty of training deep feedforward neural networks Xavier Glorot, Yoshua Bengio
\bibitem{segmentation} Recent progress in semantic image segmentation Xiaolong Liu, Zhidong Deng, Yuhan Yang
\bibitem{sequenceModelling} An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Shaojie Bai, J. Zico Kolter, Vladlen Koltun
\bibitem{bn}Sergey Ioffe, Christian Szegedy Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://arxiv.org/abs/1502.03167
\bibitem{fcn}Fully Convolutional Networks for Semantic Segmentation Jonathan Long, Evan Shelhamer, Trevor Darrell
\bibitem{DSN}Deeply-Supervised Nets Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu
\bibitem{denseNet}Densely Connected Convolutional Networks Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger
\bibitem{rnn} A., Servan-Schreiber, D., and McClelland, J. L. Finite-state automata and simple recurrent networks Neural Computation, pp , 1:372-381.
\bibitem{rnnDifficulty} Y. Bengio, P. Simard, P. Frasconi Learning long-term dependencies with gradient descent is difficult, IEEE Transactions on Neural Networks (Volume: 5 ,Issue: 2 , March 1994)
\bibitem{lstmSpeech} Alex Graves, Navdeep Jaitly Towards End-to-End Speech Recognition with Recurrent Neural Networks
\bibitem{lstmVideo} Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko,  Sequence to Sequence -- Video to Text
\bibitem{lstmHandwriting} Alex Graves, Jurgen Schmidhuber Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks
\bibitem{lstmTranslation} Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
\bibitem{fcnTranslation}Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction Maha Elbayad, Laurent Besacier, Jakob Verbeek
\bibitem{lstm} Sepp Hochreiter; Jürgen Schmidhuber Long short-term memory. Neural Computation
\bibitem{tcn} Nal Kalchbrenner  Lasse Espeholt  Karen Simonyan  Aaron van den Oord  Alex Graves  Koray Kavukcuoglu Neural Machine Translation in Linear Time
\bibitem{wavenet}Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu WaveNet: A Generative Model for Raw Audio
\bibitem{glu} Yann N. Dauphin, Angela Fan, Michael Auli, David Grangie Language Modeling with Gated Convolutional Network, arXiv:1612.08083v3
\bibitem{wavenetReview} Boilard, Jonathan \& Gournay, Philippe \& Lefebvre, R.. (2019). A Literature Review of WaveNet: Theory, Application and Optimization. 
\bibitem{ctc}Alex Graves, Santiago Fernandez, Faustino Gomez, Jurgen Schmidhuber, Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks
\bibitem{adam}Kingma, D.P., \& Ba, J. (2014). Adam: A Method for Stochastic Optimization. CoRR, abs/1412.6980.
\bibitem{tensorflow}Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.
\bibitem{minimap} Li, H. (2018). Minimap2: pairwise alignment for nucleotide sequences. Bioinformatics, 34:3094-3100. doi:10.1093/bioinformatics/bty191
\end{thebibliography}

\thispagestyle{empty}
\pagenumbering{gobble}



% --- 7. Wykaz symboli i skrótów - jeśli nie ma, zakomentować

% ----- 8. Spis rysunków - jeśli nie ma, zakomentować --------
\listoffigures
\thispagestyle{empty}


% ------------ 9. Spis tabel - jak wyżej ------------------
\renewcommand{\listtablename}{Spis tabel}
\listoftables
\thispagestyle{empty}


% 10. Spis załączników - jak nie ma załączników, to zakomentować lub usunąć

\thispagestyle{empty}


\end{document}
