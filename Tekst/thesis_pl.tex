\documentclass[a4paper,11pt,twoside]{report}
% KOMPILOWAĆ ZA POMOCĄ pdfLaTeXa, PRZEZ XeLaTeXa MOŻE NIE BYĆ POLSKICH ZNAKÓW

% -------------- Kodowanie znaków, język polski -------------

\usepackage[utf8]{inputenc}
\usepackage[MeX]{polski}
\usepackage[T1]{fontenc}
\usepackage[english,polish]{babel}


\usepackage{amsmath, amsfonts, amsthm, latexsym} % głównie symbole matematyczne, środowiska twierdzeń

\usepackage[final]{pdfpages} % inputowanie pdfa
%\usepackage[backend=bibtex, style=verbose-trad2]{biblatex}


% ---------------- Marginesy, akapity, interlinia ------------------

\usepackage[inner=20mm, outer=20mm, bindingoffset=10mm, top=25mm, bottom=25mm]{geometry}


\linespread{1.5}
\allowdisplaybreaks

\usepackage{indentfirst} % opcjonalnie; pierwszy akapit z wcięciem
\setlength{\parindent}{5mm}


%--------------------------- ŻYWA PAGINA ------------------------

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
% numery stron: lewa do lewego, prawa do prawego 
\fancyfoot[LE,RO]{\thepage} 
% prawa pagina: zawartość \rightmark do lewego, wewnętrznego (marginesu) 
\fancyhead[LO]{\sc \nouppercase{\rightmark}}
% lewa pagina: zawartość \leftmark do prawego, wewnętrznego (marginesu) 
\fancyhead[RE]{\sc \leftmark}

\renewcommand{\chaptermark}[1]{
\markboth{\thechapter.\ #1}{}}

% kreski oddzielające paginy (górną i dolną):
\renewcommand{\headrulewidth}{0 pt} % 0 - nie ma, 0.5 - jest linia


\fancypagestyle{plain}{% to definiuje wygląd pierwszej strony nowego rozdziału - obecnie tylko numeracja
  \fancyhf{}%
  \fancyfoot[LE,RO]{\thepage}%
  
  \renewcommand{\headrulewidth}{0pt}% Line at the header invisible
  \renewcommand{\footrulewidth}{0.0pt}
}



% ---------------- Nagłówki rozdziałów ---------------------

\usepackage{titlesec}
\titleformat{\chapter}%[display]
  {\normalfont\Large \bfseries}
  {\thechapter.}{1ex}{\Large}

\titleformat{\section}
  {\normalfont\large\bfseries}
  {\thesection.}{1ex}{}
\titlespacing{\section}{0pt}{30pt}{20pt} 
%\titlespacing{\co}{akapit}{ile przed}{ile po} 
    
\titleformat{\subsection}
  {\normalfont \bfseries}
  {\thesubsection.}{1ex}{}


% ----------------------- Spis treści ---------------------------
\def\cleardoublepage{\clearpage\if@twoside
\ifodd\c@page\else\hbox{}\thispagestyle{empty}\newpage
\if@twocolumn\hbox{}\newpage\fi\fi\fi}


% kropki dla chapterów
\usepackage{etoolbox}
\makeatletter
\patchcmd{\l@chapter}
  {\hfil}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill}
  {}{}
\makeatother

\usepackage{titletoc}
\makeatletter
\titlecontents{chapter}% <section-type>
  [0pt]% <left>
  {}% <above-code>
  {\bfseries \thecontentslabel.\quad}% <numbered-entry-format>
  {\bfseries}% <numberless-entry-format>
  {\bfseries\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}% <filler-page-format>

\titlecontents{section}
  [1em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}

\titlecontents{subsection}
  [2em]
  {}
  {\thecontentslabel.\quad}
  {}
  {\leaders\hbox{\normalfont$\m@th\mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$}\hfill\contentspage}
\makeatother



% ---------------------- Spisy tabel i obrazków ----------------------

\renewcommand*{\thetable}{\arabic{chapter}.\arabic{table}}
\renewcommand*{\thefigure}{\arabic{chapter}.\arabic{figure}}
%\let\c@table\c@figure % jeśli włączone, numeruje tabele i obrazki razem


% --------------------- Definicje, twierdzenia etc. ---------------


\makeatletter
\newtheoremstyle{definition}%    % Name
{3ex}%                          % Space above
{3ex}%                          % Space below
{\upshape}%                      % Body font
{}%                              % Indent amount
{\bfseries}%                     % Theorem head font
{.}%                             % Punctuation after theorem head
{.5em}%                            % Space after theorem head, ' ', or \newline
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}%  % Theorem head spec (can be left empty, meaning `normal')
\makeatother

% ----------------------------- POLSKI --------------------------------

\theoremstyle{definition}
\newtheorem{theorem}{Twierdzenie}[chapter]
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{example}[theorem]{Przykład}
\newtheorem{proposition}[theorem]{Stwierdzenie}
\newtheorem{corollary}[theorem]{Wniosek}
\newtheorem{definition}[theorem]{Definicja}
\newtheorem{remark}[theorem]{Uwaga}



% ----------------------------- Dowód -----------------------------

%\makeatletter
%\renewenvironment{proof}[1][\proofname]
%{\par
%  \vspace{-12pt}% remove the space after the theorem
%  \pushQED{\qed}%
%  \normalfont
%  \topsep0pt \partopsep0pt % no space before
%  \trivlist
%  \item[\hskip\labelsep
%        \sc
%    #1\@addpunct{:}]\ignorespaces
%}
%{%
%  \popQED\endtrivlist\@endpefalse
%  \addvspace{20pt} % some space after
%}
%
%\renewcommand{\qedhere}{\hfill \qedsymbol}
%\makeatother





% -------------------------- POCZĄTEK --------------------------


% --------------------- Ustawienia użytkownika ------------------

\newcommand{\tytul}{Tytuł pracy dyplomowej w języku polskim}
\renewcommand{\title}{English title}
\newcommand{\type}{inżyniers} % magisters, licencjac
\newcommand{\supervisor}{dr inż. Promotor X}



\begin{document}
\sloppy

\includepdf[pages=-]{strona_tytulowa-jeden-autor.pdf}


% ------------------ STRONA Z PODPISAMI AUTORA/AUTORÓW I PROMOTORA ------------------


\thispagestyle{empty}\newpage
\null

\vfill

\begin{center}
\begin{tabular}[t]{ccc}

............................................. & \hspace*{100pt} & .............................................\\
podpis promotora & \hspace*{100pt} & podpis autora


\end{tabular}
\end{center}



% ---------------------------- ABSTRAKTY -----------------------------
% W PRACY PO POLSKU, NAPIERW STRESZCZENIE PL, POTEM ABSTRACT EN

{
\begin{abstract}

\begin{center}
\tytul
\end{center}

Streszczam.

Lorem ipsum dolor sit amet, consetetur sadipscing elit, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\\

\noindent \textbf{Słowa kluczowe:} slowo1, slowo2, ...
\end{abstract}
}

\null\thispagestyle{empty}\newpage

{\selectlanguage{english}
\begin{abstract}

\begin{center}
\title
\end{center}

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumyeirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diamvoluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.\\

\noindent \textbf{Keywords:} keyword1, keyword2, ...
\end{abstract}
}


% --------------------- OŚWIADCZENIE -----------------------------------------


\null\thispagestyle{empty}\newpage

\null \hfill Warszawa, dnia ..................\\

\par\vspace{5cm}

\begin{center}
Oświadczenie
\end{center}

\indent Oświadczam, że pracę \type ką pod
tytułem ,,\tytul '', której promotorem jest \supervisor , wykonałam/wykonałem
samodzielnie, co poświadczam własnoręcznym podpisem.
\vspace{2cm}


\begin{flushright}
  \begin{minipage}{50mm}
    \begin{center}
      ..............................................

    \end{center}
  \end{minipage}
\end{flushright}

\thispagestyle{empty}
\newpage

\null\thispagestyle{empty}\newpage


% ------------------- 4. Spis treści ---------------------
\pagenumbering{gobble}
\tableofcontents
\thispagestyle{empty}

\newpage % JEŻELI SPIS TREŚCI MA PARZYSTĄ LICZBĘ STRON, ZAKOMENTOWAĆ
% ALBO JAK KTOŚ WOLI WTEDY DWIE STRONY ODSTĘPU, DODAĆ \null\newpage

% -------------- 5. ZASADNICZA CZĘŚĆ PRACY --------------------
\null\thispagestyle{empty}\newpage
\pagestyle{fancy}
\pagenumbering{arabic}
\setcounter{page}{11} % JEŻELI Z POWODU DUŻEJ ILOŚCI STRON W SPISIE TREŚCI SIĘ NIE ZGADZA, TRZEBA ZMODYFIKOWAĆ RĘCZNIE

\chapter*{Wstęp}
\markboth{}{Wstęp}
\addcontentsline{toc}{chapter}{Wstęp}

\chapter*{Opis problemu}

\section*{DNA}

DNA to molekuła składająca się z dwóch zwiniętych siebie nici tworzących podwójną helisę. DNA jest nośnikiem informacji genetycznej, przenoszącej instrukcje dotyczące rozwoju, funkcjonowania i reprodukcji wszystkich znanych organizmów żyjących i niektórych wirusów. Podstawowym składnikiem budującym DNA są nukleotydy. Każdy z nich zawiera jedną z czterech z zasad: adeninę, cytozynę, guaninę lub tyminę, często oznaczane pierwszą literą nazwy (odpowiednio A, C, G lub T). Sekwencja tych baz w DNA definiuje kod genetyczny. Kompletny kod genetyczny organizmu jest nazywany genomem. Długość genomu znacznie różni się pomiędzy gatunkami.


\begin{table}
\centering
\begin{tabular}{ll}
	Bakteria e. coli                                                                           & $\sim$4.6 miliona  \\
	\begin{tabular}[c]{@{}l@{}}Schizosaccharomyces pombe\\ (grzyb jednokomórkowy)\end{tabular} & $\sim$12.5 miliona \\
	\begin{tabular}[c]{@{}l@{}}Drosophila melanogaster\\ (muszka owocowa)\end{tabular}         & $\sim$170 milionów \\
	Oryza sativa (ryż)                                                                         & $\sim$470 milionów \\
	Canis familiaris (pies)                                                                    & $\sim$2.4 miliarda \\
	Homo sapiens (czlowiek)                                                                    & $\sim$2.9 miliarda
\end{tabular}
\caption{\label{}Liczba par zasad w genomach wybranych organizmów.}
\end{table}

\section*{Sekwencjonowanie DNA}

Sekwencjonowanie DNA to proces rozpoznawania sekwencji DNA danego organizmu. Wczesne metody sekwencjonowania (np. metoda Sangera opracowana w 1977 r. \cite{sequencingTechnologies}) były zbyt kosztowne do powszechnych zastosowań, lecz metody drugiej i trzeciej generacji umożliwiły zbadanie DNA organizmów na szerszą skalę i wykorzystanie tej wiedzy w celach takich jak badanie odporności na nowotwory\cite{cancerImmunity} i przyczyn zachorowań na nie\cite{cancerNonCoding}, badanie historii adaptacji w procesie ewolucji człowieka \cite{adaptation}, medycyna spersonalizowana do kodu genetycznego człowieka \cite{personalizedMedicine}, badania prenatalne \cite{prenatal} i diagnostyka chorób o podłożu genetycznym\cite{diagnosis}.

Wynikiem sekwencjonowania jest fragment całego genomu. nazywanym odczytem(\textit{read}). Długość odczytów jest zależna od użytej technologii - sekwencjonowanie technologią produkuje odczyty o długości do 600 par zasad przy celności do 99.9\%, a przy użyciu technologii PacBio odczyty osiągają długość nawet do 100 tysięcy par zasad przy celności 87\%. Odczyty są dopasowywane i łączone tak aby otrzymać cały genom(po angielsku ten proces jest nazywany \textit{genome assembly}).  Pomimo większej ilości błędów w długich odczytach są one potrzebne do dopasowania regionów repetytywnych, czyli podsekwencji genomu występujących w nim wielokrotnie. Dla tego do składania genomu wykorzystuje się zarówno odczyty długie i krótkie.

\section*{Oxford Nanopore MinION}

Oxford Nanopore MinION to technologia sekwencjonowania dostępna publicznie od maja 2015 r. produkująca długie odczyty, nawet do 2 milionów par zasad. Wykorzystuje ona pory przez, które przepływają pojedyncze nici DNA. Pory osadzone są w rezystentnej membranie polimerowej, przez którą przepływa prąd elektryczny. Nukleotyd znajdujący się w porze wpływa na jego opór elektryczny, tym samym powodując zmianę napięcia. Sygnał elektryczny jest mierzony i to on jest rezultatem pracy urządzenia. Analizując go jest możliwe uzyskanie sekwencji DNA nici, która przepłynęła przez por. Proces ten nazywany jest basecallingiem. Na sygnał elektryczny wpływają pojedyncze molekuły znajdujące się wewnątrz poru co wprowadza wiele szumu. Dodatkowo w najwęższym punkcie poru znajduje się jednocześnie 4-5 nukleotydów i wszystkie z nich wpływają na opór elektryczny poru i tym samym mierzony sygnał. Zatem basecalling jest złożonym problemem, który ma kluczowe znaczenie dla jakości sekwencjonowania nanoporowego.

Urządzenie MinION jest bardzo przenośne (ma rozmiar zszywacza) i jego koszt jest znacznie mniejszy niż konkurencyjnych urządzeń. Było ono wykorzystywane nawet w terenie, na przykład podczas badania ognisk choroby w odległych rejonach\cite{ebola}. Niski koszt urządzenia pozaola na zastosowanie go na szerszą skalę niż innych technologii sekwencjonowania.

\section*{Istniejące rozwiązania}

Oxford Nanopore Technologies (producent MinIONa) oferuje kilka basecallerów. Są to oficjalny basecaller Albacore, Guppy wykorzystujący procesory graficzne i Scrappie określany przez producenta jako "demonstracja technologii". Aktualne wersje Albacore i Guppy wykorzystują sieci neuronowe. W przeszłości ONT oferowało również NanoNet będący pierwszym basecallerem opartym o sieci neuronowe, a także Metrichor działający w chmurze obliczeniowej oparty o łańcuchy Markowa. 

Opublikowano również następujące basecallery z otwartym kodem źródłowym pochodzące od społeczności akademickiej.

DeepNano\cite{deepNano} wykonuje proces basecallingu w dwóch krokach. W pierwszym sygnał jest dzielony na fragmenty odpowiadające k-merom. W drugim dwukierunkowa rekurencyjna sieć neuronowa wykorzystuje statystyki powstałych w pierwszym kroku fragmentów do przewidzenia k-meru odpowiadającemu każdemu fragmentowi. 

BasecRAWller\cite{basecrawler} wykorzystuje dwie rekurencyjne sieci neuronowe. Pierwsza z nich przewiduje dla każdego kroku czasowego w sygnale wejściowym prawdopodobieństwo tego, że jest on granicą pomiędzy segmentom sygnału odpowiadającego kolejnym nukleotydom. Druga mapuje w ten sposób powstałe segmenty na odpowiadające nim bazy.

Chiron\cite{chiron} tłumaczy sygnał elektryczny bezpośrednio na sekwencję DNA. Wykorzystuje on model oparty o sieci konwolucyjne, sieci rekurencyjne i dekodowanie CTC.

Jakość basecallingu można ocenić wykorzystując celność pojedynczego odczytu lub celność konsensusu utworzonego z wielu odczytów pochodzących z tego samego rejonu genomu. Konsensus powstaje poprzez nałożenie na siebie wielu odczytów z pasującymi do siebie fragmentami. Obie te metryki oblicza się poprzez mapowanie uzyskanej sekwencji do znanej sekwencji referencyjnej. Nie muszą być one skorelowane - odczyty o dużej liczbie losowych błędów mogą stworzyć konsensus o bardzo dużej celności, ponieważ losowe błędy zostaną wyeliminowane poprzez nałożenie na siebie odczytów. Natomiast systematyczne błędy zostaną powielone w konsensusie.

Literatura porównująca basecallery pokazuje, że średnia dokładność otrzymanych odczytów jest w przedziale 80-90\%, a celność konsensus otrzymanego z wielu odczytów wacha się pomiędzy 99.37\% a 99.94\%\cite{wick}. Zatem w genomie bakterii e. coli o długości około 4.6 miliona baz wystąpiłoby co najmniej 4.6 tysiąca insercji, delecji i substytucji. Taka liczba substytucji znacznie utrudnia użycie sekwencjonowania nanopore w zastosowaniach gdzie nawet zamiana pojedynczej bazy w genomie, nazywana w skrócie SNP(single nucleotide polymorphism), ma kluczowe znaczenie\cite{snp}. Zatem stworzenie basecallera o lepszej precyzji zwiększyłoby zakres zastosowań technologii.


\begin{table}
	\centering
	\begin{tabular}{llll}
		Basecaller   & Mediana celności odczytu & Mediana celności konsensusu & \begin{tabular}[c]{@{}l@{}}Średnia długość odczytu \\ względem referencji\end{tabular} \\
		Albacore     & 88\%                     & 99.37\%                     & 99.8\%                                                                                 \\
		Guppy        & 88\%                     & 99.37\%                     & 99.8\%                                                                                 \\
		Scrappie     & 86\%                     & 99.42\%                     & 95\%                                                                                   \\
		Chiron       & 86\%                     & 99.4\%                      & 98\%                                                                                   \\
		DeepNano     & 78\%                     & 96\%                        & 99\%                                                                                   \\
		basecRAWller & 74\%                     & 85\%                        & 106\%                                                                                 
	\end{tabular}
	\caption{\label{}Porównanie jakości dostępnych Basecallerów. Wyniki pochodzą z basecallingu danych z sekwencjowania \textit{K. pneumoniae}\cite{wick}}
\end{table}

\chapter*{Opis rozwiązania}

\section*{Przygotowanie zbiorów danych danych}

W celu nauczenia i testowania sieci neuronowej przygotowane zostały 2 zbiory danych w postaci par \textit{wektor sygnału, odpowiadające mu nukleotydy}.
Pierwszy zbiór powstał z danych z sekwencjonowania genomu ludzkiego z linii komórkowej NA12878. Wykorzystano DNA pochodzące z chromosomu mitochondrialnego, na które składa się 5000 odczytów.
Dane pochodzące z sekwencjonowania metodą Nanopore pochodzą z repozytorium udostępnionego przez Whole Human Genome Sequencing Project\cite{nanoporeHuman},a genom referencyjny sekwencjonowany metodą Illumina pochodzi z The International Genome Sample Resource\cite{refGenome}. Aby uzyskać dane treningowe nie będące obciążone błędem innego \textit{basecallera} wykorzystano pakiet \textit{nanoraw}\cite{nanoraw}. Dokonuje on uliniowienia sygnału pochodzącego z sekwencjonowania technologią nanopore i genomu referencyjnego. W efekcie otrzymano podział sygnału na fragmenty, z których każdy odpowiada jednemu z nukleotydów.

W celu poprawienia generalności rozwiązania do uczenia sieci wykorzystano również zbiór danych z sewkwencjonowania gatunków \textit{Escherichia virus Lambda} i \textit{Escherichia coli} składający się z 4000 odczytów \cite{chironData}. Uliniowienie z genomami referencyjnymi zostało wykonane przez autorów zbioru, w sposób analogiczny do opisanego powyżej.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{squiggle}
	\caption{Wizualizacja dopasowania baz do znormalizowanego sygnału}
\end{figure}

\section*{Eksploracyjna analiza danych}

Sygnał elektryczny o całkowitej długości 246 892 693 został dopasowany do sekwencji składających się w sumie z 29 193 641 par zasad. Zatem długość segmentu odpowiadającego jednej bazie wynosi 8.457071. Długość takich segmentów ma kluczowe znaczenie w budowie modelu, ponieważ na poziom sygnału w danym kroku czasowym ma wpływ 4 do 5 baz obecnie znajdujących się w porze. Zatem, aby wykonać poprawną predykcję model potrzebuje informacji o bazach w odległości 2 od tej, dla której dokonywana jest predykcja. Średnia długość fragmentu sygnału odpowiadającego 5-merowi wyniosła 33.81744, z odchyleniem standardowym równym 21.70463. Tak duże odchylenie standardowe wynika z obecności małej liczby 5-merów o sygnale bardzo dużej długości - 99\% fragmentów sygnału odpowiadających 5-merowi miało długość mniejszą niż 79, 99.95\% mniejszą niż 209, a 99.9\% niż 620. Najdłuższy taki fragment sygnału miał długość 10197. Podczas sekwencjonowania nić dna przepływa przez por z prędkością 250-450 baz na sekundę a sygnał jest próbkowany 4000 razy na sekundę. Zatem na jeden 5-mer powinno przypadać do 80 próbek sygnału. Większa ich liczba może świadczyć o zablokowaniu się nici w porze lub innej anomalii w procesie sekwencjonowania. Ponad 99\% 5-merów w danych treningowych spełnia ten warunek.

\begin{table}[]
	\centering
	\begin{tabular}{ll}
		10\%    & 21  \\
		30\%    & 26  \\
		50\%    & 31  \\
		70\%    & 37  \\
		90\%    & 49  \\
		99\%    & 79  \\
		99.5\%  & 94  \\
		99.9\%  & 155 \\
		99.99\% & 620
	\end{tabular}
	\caption{\label{}Wybrane kwantyle długośći syngałów odpowiadających 5-merow nukleotydów.}
\end{table}

\section*{Ogólna architektura rozwiązania}

Problem basecallingu polega na translacji pewnej sekwencji wejściowej na sekwencję wejściową w innej domenie o zmiennej ale mniejszej długości. Ma on wiele podobieństw do problemów takich jak rozpoznawanie mowy, rozpoznawanie pisma pisanego, segmentacja obrazu czy tłumaczenie maszynowe, do których z sukcesami zastosowano sieci neuronowe i uczenie głębokie. Modele i techniki użyte do tych zastosowań mogą być skuteczne w dziedzinie basecallingu.

Z racji na dużą długość odczytów sygnał odpowiadający całemu odczytowi musi zostać podzielony na mniejsze fragmenty, które mogą zostać przetworzone przez sieć. Każdy z tych segmentów powinien być wystarczająco długi, aby odpowiadać kilku 5-merom nukleotydów. Z sygnału powinny zostać wyekstrahowane pewne cechy lokalne względem kroku czasowego. Sieć powinna zawierać również moduł, który znajdzie zależności pomiędzy tymi cechami, aby dokonać segmentacji i dopasować sygnał do baz. Moduł ten powinien wykorzystywać cechy dla wszystkich kroków czasowych, wchodzących w skład 5-meru do którego należy obecny krok czasowy. Na podstawie wyniku obliczeń tego modułu zostanie obliczone prawdopodobieństwo, że próbka sygnału z kroku czasowego odpowiada jednej z baz lub granicy pomiędzy nimi. Z racji tego, że długość sekwencji wyjściowej jest zmienna należy wykorzystać dekodowanie CTC\cite{ctc}. Wyniki obliczone dla każdego krótkiego fragmentu muszą zostać złożone w konsensus reprezentujący sekwencję DNA dla całego odczytu.

\begin{figure}[h!]
	\centering
	\includegraphics[]{architecture}
	\caption{Schemat wysokopoziomowej architektury rozwiązania.}
\end{figure}


\section*{Reprezentacja i przygotowanie danych wejściowych}

Sygnał jest normalizowany poprzez odjęcie średniej dla odczytu i podzielenie przez odchylenie standardowe. Sygnał pochodzący z jednego odczytu. jest dzielony na fragmenty po 300 próbek. Zgodnie z przedstawionymi wcześniej kwantylami długości fragmentów sygnału odpowiadającym 5-merom podsekwencje o takiej długości w ponad 99.9\% przypadków odpowiadają co najmniej kilku 5-merom. Zastosowanie takiej długości umożliwia również uczenie sieci w batchach o wielkości ponad 100 przykładów co przyspiesza proces uczenia. 

\section{Wykorzystane koncepty z dziedziny uczenia głębokiego}

W kolejnych sekcjach opisano techniki uczenia głębokiego i architektury sieci, które wykorzystano w testowanych modelach lub były one inspiracją dla nich.

\subsection{ReLU}

Funkcje aktywacji są stosowane w sieciach neuronowych w celu dodania nieliniowości do modelu. Są one stosowane do wyniku obliczeń warstw wewnątrz sieci. \textit{Recitified linear unit} czyli ReLU jest funkcją aktywacji daną wzorem
\[Relu(x) = max(0,x)\]

\subsection{\textit{Batch Normalization}}

\textit{Batch normalization} to metoda przyspieszająca uczenie sieci neuronowych, polegająca na normalizacji danych wejściowych do warstw ukrytych. Dzięki temu rozkład wartości aktywacji tych warstw pozostaje taki sam podczas całego procesu uczenia sieci. Wynik zastosowania \textit{Batch normalization } dla danych wejściowy $h$ jest danym wzorem:
\[BN(h, \lambda, \beta)=\beta+\lambda\frac{h-\hat{E}}{\sqrt{\hat{Var}(h)+\epsilon}}\]
gdzie $\lambda$ i $\beta$ są wagami nauczonymi przez sieć a $\epsilon$ to parametr regularyzujący.

\subsection*{Sieci konwolucyjne, ResNet}

Sieci konwolucyjne(splotowe) to klasa głębokich sieci neuronowych wykorzystujących operację konwolucji. Charakteryzują się one mniejszą ilością parametrów i lepszą umiejętnością uczenia się lokalnych cech danych wejściowych niż sieci w pełni połączone\cite{cnn} i dzięki temu znajdują zastosowanie w ekstrakcji cech w problemach, w których dane wejściowe są zależne od położenia lub czasu, takich jak rozpoznawanie obrazu \cite{vgg}, wideo\cite{cnnVideo} lub dźwięku\cite{cnnAudio}.

W przypadku sieci konwolucyjnych składających się z wielu warstw występuje problem zanikającego gradientu, co znacznie spowalnia lub uniemożliwia uczenia takich sieci\cite{difficulty}.
Architekturą, która próbuje rozwiązać tą trudność jest \textit{resNet}, osiągający jedne z najlepszych wyników w dziedzinie klasyfikacji obrazu. Sieci tego typu składają się z bloków zawierających połączenia omijające część warstw. Na wyjściu z każdego bloku znajduje się suma danych wejściowych i danych przetworzonych przez warstwy wewnątrz bloku. Dzięki temu sygnał nie zanika przed osiągnięciem dalszej części sieci. Umożliwiło to trenowanie sieci o większej liczbie warstw bez utraty celności predykcji.\cite{resnet}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{resnet}
	\caption{Schemat połączeń w sieci ResNet składającej się z 3 bloków.}
\end{figure}

Ulepszona wersja tej architektury zawierająca mapowania identycznościowe została zaprezentowana w \cite{preResnet}. Autorzy proponują taką modyfikację sieci, aby dane wejściowe były dodawane do kolejnych warstw bez żadnych modyfikacji. We wcześniejszej wersji do danych wejściowych stosowano funkcje aktywacji i funkcje normalizujące. Zapobiega to zanikaniu gradientu nawet w sieciach składających się z tysiąca warstw. Dzięki temu można uniknąć efektu pogarszania się jakości predykcji wraz ze zwiększaniem ilości warstw w sieci powyżej określonej liczby.

\begin{figure}[h!]
	\centering
	\includegraphics{resnetBlock}
	\caption{Konstrukcja bloku użytego w sieci resNet w wersji a) oryginalnej i b) z mapowaniem identycznościowym}
\end{figure}

\subsection*{DenseNet}

W architekturze DenseNet, będącej rozwinięciem idei wykorzystanych w ResNet, każda z warstw przekazuje do dalszej części sieci skonkatenowane wyniki obliczeń wykonanych przez siebie i każdą z poprzednich warstw. Na wyjściu z sieci znajdują sie cechy obliczone, przez każdą z warstw w sieci - wynikami kolejnych warstw są cechy danych wejściowych coraz wyższego rzędu. Dzięki temu wszystkie warstwy w sieci podlegają nadzorowi, podobnie jak w sieciach DSN(Deeply-Supervised Nets)\cite{DSN}. Wykorzystanie cech obliczonych w początkowych warstwach w dalszej części sieci, umożliwia zastosowanie modeli o mniejszej liczbie parametrów. Klasyfikacja jest dokonywana na podstawie cech niskiego i wysokiego rzędu. Dzięki temu sieci typu DenseNet osiągają lepszą precyzję klasyfikacji wielu problemach niż sieci typu ResNet, mimo zastosowania mniej złożonego modelu\cite{denseNet}. 

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{densenet}
	\caption{Schemat połączeń w sieci DenseNet składającej się z 3 bloków.}
	\label{fig:denseNet}
\end{figure}

\subsection*{Sieci rekurencyjne, LSTM}

Sieci rekurencyjne umożliwiają przetwarzanie danych zależnych od czasu, wykorzystująe do obliczenia wyniku dla pewnego kroku czasowego informacji obecnych w poprzednich krokach. Składają się one z komórek, jednej dla kroku czasowego, z których każda przyjmuje na wejściu stan obliczony przez poprzednią komórkę i dane wejściowe dla kroku czasowego, zwraca wynik i oblicza stan, który jest przekazywany do następnej komórki. Ponieważ wagi są dzielone pomiędzone krokami czasowymi sieci rekurencyjne można interpretować jako wykonanie obliczeń w pętli przez sieć składającą się z jednej takiej komórki. Dzięki temu że stan jest przekazywany pomiędzy kolejnymi krokami sieci rekurencyjne mają mechanizm pamięci i mogą znajdować zależności pomiędzy danymi oddalonymi od siebie o wiele kroków czasowych. \cite{rnn}

\begin{figure}[h!]
	\centering
	\includegraphics{rnn}
	\caption{Sieć rekurencyjna w postaci rozwiniętej i w postaci pętli}
\end{figure}

W praktyce sieciom rekurencyjnym nie udawało się nauczyć się zależności pomiędzy danymi oddalonymi od siebie o więcej niż kilka kroków czasowych \cite{rnnDifficulty}. Rozwiązaniem tego problemu okazały się sieci LSTM(Long short-term memory) wykorzystujące specjalną konstrukcję komórki \cite{lstm}. Składa się ona z czterech warstw i przekazuje do kolejnej komórki jako stan dwie zmienne $h_t$ i $C_t$ opowiadające pamięci krótko i długotrwałej.  

Komórka LSTM dla kroku czasowego $t$ wykonuje następujące operacje:
\begin{itemize}
	\item Na podstawie stanu $h_{t-1}$ i danych wejściowych $x_t$ obliczany jest wektor $f_t$ o wartościach z przedziału od 0 do 1, który reprezentuje, które wartości ze stanu $C_{t-1}$ powinny zostać zapomniane. 
	\item Wektor $g_t$ powstaje na podstawie $h_{t-1}$ i $x_t$. Reprezentuje on zmienne dodane do pamięci długotrwałej.
	\item Stan $C_t$ jest obliczony przez pomnożenie wektora $C_{t-1}$ przez $f_t$ a następnie dodanie $g_t$
	\item Na podstawie wektorów stanu $C_t$ i $h_{t-1}$ i danych wejściowych $x_t$ zostaje obliczony wektor $h_t$ będący jednocześnie wynikiem dla kroku czasowego $t$ i jednym z dwóch stanów przekazanych do następnej komórki
\end{itemize}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{lstm}
	\caption{Konstrukcja komórki sieci LSTM}
\end{figure}

Sieci LSTM są wykorzystywane w problemach takich jak rozpoznawanie mowy\cite{lstmSpeech}, analiza wideo\cite{lstmVideo}, rozpoznawanie pisma \cite{lstmHandwriting} i tłumaczenie maszynowe\cite{lstmTranslation}.

\subsection*{Sieci w pełni konwolucyjne}

Sieci w pełni konwolucyjne to rodzaj sieci składających się wyłącznie z warstw konwolucyjnych, które mogą być zastosowane w problemach w których rozmiar danych wyjściowych, jest zależny od rozmiaru danych wejściowych, takich jak segmentacja obrazu\cite{segmentation} czy modelowanie sekwencji np. dzwięku i tekstu \cite{sequenceModelling}. Wykorzystują one fakt, że każdy neuron w sieci przetwarza tylko dane wejściowe znajdującym się w w jego otoczeniu, którego rozmiar zależy od rozmiaru filtrów w poprzedzających go warstwach. Zatem w sieci składające się wyłącznie z warstw konwolucyjnych, liczba neuronów w ostatniej warstwie jest zależna od rozmiaru danych wejśćiowych i każdy z tych neuronów odpowiada, pewnemu elementowi z danych wejściowych, np. krokowi czasowemu lub pikselowi w obrazie\cite{fcn}.

Czasowe sieci konwolucyjne(\textit{temporal convolutional networks}) to sieci w pełni konwolucyjne dostosowane do problemów, w których dane wejściowe zależą od czasu. Stosuje się w nich konwolucje przyczynowe(causal convolutions), czyli takie, w których predykcja dla kroku czasowego \textit{t} zależy tylko od danych wejściowych dla kroku czasowego \textit{t} i wcześniejszych. Aby zwiększyć pole recepcyjne wykorzystano konwolucje rozszerzone (\textit{dilated convolutions}). Polega ona na wykorzystaniu wartości oddalonych o \textit{d} kroków czasowych przy obliczaniu wartości filtra, gdzie \textit{d} jest parametrem. Paramter ten jest zwiększany wykładniczo w kolejnych warstwach, co pozwala na wykorzystanie w predykcji danych dla oddalonych kroków czasowych.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{tcn}
	\caption{Schemat przedstawiający połaczenia w sieci używającej konwolucji czasowych i przyczynowych.}
\end{figure}

Czasowe sieci konwolucyjne znalazły zastosowanie w problemach, w których dane wejśćiowe są zależne od czasu np. tłumaczenie maszynowe\cite{fcnTranslation} i synteza dźwięku. Innym typem sieci znajdujących zastosowanie w tej klasie problemów są sieci LSTM\cite{lstm}, lecz zademonstrowano, że sieci w pełni konwolucyjne są w stanie osiągać lepsze wyniki w wielu problemach, mimo mniejszego zużycia pamięci i krótszego czasu potrzebnego do wykonania obliczeń \cite{sequenceModelling}.

\subsection*{WaveNet}

Wavenet to głęboka sieć neuronowa wykorzystana oryginalnie do syntezy mowy\cite{wavenet}. Wykorzystano w niej konwolucje rozszerzone i przyczynowe tak jak w czasowych sieciach konwolucyjnych. Do wyniku obliczeń każdej z warstw konwolucyjnych stosuje się funkcję sigmoidalną i tangens hiperboliczny. Do dalszej części sieci przekazywany iloczyn wyników zastosowania tych funkcji. Podobna operacja jest wykonywana w sieciach LSTM \cite{lstm}, i tak ja tam ma za zadanie znormalizowanie cech i wybranie, które z nich mają przekazane do dalszej części sieci. Dodatkowo wykorzystano połączenia rezydualne podobne do tych w sieci ResNet\cite{resnet}, a klasyfikacja jest dokonywana na podstawie sumy cech obliczonej przez każdą z warstw.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{wavenet_block}
	\caption{Blok sieci Waveneta. Sieć składa się z pewnej liczby takich bloków połączonych w serii.}
	\label{fig:wavenet_block}
\end{figure}

Sieci Wavenet udało się wygenerować mowę bardzo podobną do ludzkiej\cite{wavenet}, znalazła ona również zastosowanie w problemach rozpoznawania mowy, redukcji hałasu i generowania muzyki\cite{wavenetReview}.

\subsection*{Dekodowanie CTC}

Zarówno sieci konwolucyjne jak i sieci LSTM mogą przyjmować dane wejściowe różnych rozmiarów, ale rozmiar danych wyjściowych jest funkcją rozmiaru danych wejściowych. W celu zbudowania modelu, który dla danych o określonym rozmiarze może zwracać sekwencje różnych długości stosuje sie dekodowanie CTC(\textit{Connectionist Temporal Classification})\cite{ctc}. Dekoder CTC przyjmuje na wejściu macierz o rozmiarze \textit{(rozmiar alfabetu wyjściowego + 1, długość sekwencji wejściowej)}. Element macierzy na pozycji  \textit{(n, t)} opisuje prawdopodobieństwo, że krok czasowy \textit{t} danych wejśćiowych odpowiada \textit{n}-temu elementowi alfabetu wyjśćiowego. Ostatni wiersz macierzy odpowiada prawdopodobieństwu wystąpienia dodatkowego symbolu - znaku pustego odpowiadającemu granicom pomiędzy kolejnymi znakami w ciągu wyjściowym. 

Sekwencje są kodowane w tej macierzy przez ścieżki, które powstają poprzez wybranie jednego znaku dla każdego kroku czasowego. Ścieżka zostaje zamieniona na sekwencję wyjściową poprzez połączenie w nieprzerwanych podciągów składających się z jednego znaku w pojedynczy symbol(AAAAAA -> A), a następnie usunięcie znaków pustych. Zatem sekwencje wyjściowe mogą być krótsze niż sekwencje wejściowe i mogą zawierać te same znaki na sąsiadujących pozycjach, w przypadku gdy w ścieżce znajduje się pomiędzy nimi znak pusty. Prawdopodobieństwo ścieżki jest zdefiniowane przez iloczyn prawdopodobieństw wybranych znaków dla każdego kroku czasowego.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.8]{ctc_array}
	\caption{Przykład dekodowania CTC dla alfabetu wyjściowego {A,C,G,T} i sekwencji wejściowej długości 6.}
\end{figure}

Prawdopodobieństwo sekwencji wyjściowej to suma prawdopodobieństw wszystkich ścieżek, które mogą zostać zamienione na tą sekwencję. Znalezienie optymalnego rozwiązania problemu wybrania ścieżki o największym prawdopodobieństwie jest zbyt złożone obliczeniowo, dlatego zamiast tego stosuje się algorytm zachłanny, który wybiera dla każdego kroku czasowego znak o największym prawdopodobieństwie lub algorytm \textit{beam search}, który dla każdego kolejnego kroku czasowego zapamiętuje określoną liczbę najlepszych kandydatów. W przypadku dokonywania predykcji przez model jako sekwencja wyjściowa wybierana jest ta znaleziona przez jeden z tych algorytmów.

Funkcja straty dla dekodowania CTC zdefiniowana jest jako
\[-\ln{(p(x|z))}\]
gdzie $p(x|z)$ to prawdopodobieństwo prawdziwej sekwencji dla danej macierzy prawdopodobieństw dla kroków czasowych.


\section*{Architektury przetestowanych sieci neuronowych}

\subsection{ResNet + LSTM z połączeniami rezydualnymi}

Na dane wejściowe nakładana jest warstwa konwolucyjna zmieniająca ich kształt tak, aby ostatni wymiar miał rozmiar 256. Jest to potrzebne, aby mogły one zostać wykorzystane w połączeniach rezydualnych. Następnie w sieci znajduje się 5 bloków rezydualnych z mapowaniem identycznośćiowym z warstwami konwolucyjnymi obliczającymi 256 cech. Po nich następuje 3 warstwowa sieć LSTM obliczająca 128 cech. Jest ona aplikowana do cech obliczonych przez poprzednie warstwy w obu kierunkach - od początku od końca i od końca do początku. Zatem każda z warstw oblicza również po 256 cech. Również w tej części sieci zostały dodane połączenia rezydualne omijające warstwę. Są one sumowane z cechami obliczonymi przez warstwę, które omijają. Zatem sygnał wejściowy może płynąć przez całą sieć bez żadnych modyfikacji poza zmianą kształtu w pierwszej warstwie. Ostatnią warstwą w sieci jest warstwa gęsto połączona która dla każdego kroku czasowego oblicza prawdopodobieństwo że odpowiada on któremuś z nukleotydów lub granicy pomiędzy nimi. Wynik tej warstwy trafia do dekodera CTC.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{resnetLstm}
	\caption{Architektura modelu ResNet + LSTM z połączeniami rezydualnymi}
\end{figure}

\subsection{ResNet + Konwolucje czasowe}

Podobnie jak w poprzedniej sieci do ekstrakcji cech lokalnych wykorzystano 5 bloków ResNet. Po nich następują bloki wykorzystujące warstwy konwolucyjne rozszerzone i przyczynowe. W każdym bloku znajdują się 4 warstwy. Najpierw aplikowane są dwie z nich - jedna w oryginalnym kierunku a druga w odwróconym. Efekt zastosowania warstwy w odwróconym kierunku jest uzyskany poprzez odwróceniu danych wejściowych wobec wymiaru odpowiadającemu czasowi, zastosowanie warstwy i odwróceniu otrzymanego wyniku. Oba te wyniki są konkatenowane, a następnie stosuje się do nich \textit{Batch normalization} i funkcję aktywacyjną ReLu Dwie kolejne warstwy wykonują tą samą operację. Każda z warstw oblicza po 128 cech. Wszystkie warstwy wewnątrz danego bloku mają tą samą wartość rozszerzenia(dilation), lecz w każdym kolejnym bloku jest ona podwajana. W sumie w sieci znajduje się 7 takich bloków, czyli ostatni ma wartość rozszerzenia równą 64. Jądro ma wielkość 3, zatem pole receptywne konwolucji w ostatniej warstwie sięga 128 kroków czasowych w obu kierunkach. Warstwa gęsto połączona jest aplikowana do sumy wyników obliczeń każdego z bloków.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{tcn_block}
	\caption{Budowa bloku wykorzystującego konwolucje czasowe}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{resnetTcn}
	\caption{Architektura modelu ResNet + Konwolucje czasowe}
\end{figure}

\subsection{ResNet + WaveNet}

Ten model jest rozszerzeniem poprzedniego o idee wykorzystane w WaveNet\cite{wavenet}. Do ekstrakcji cech lokalnych wykorzystano 5 bloków ResNet. Po nich następują bloki zawierające 2 warstwy konwolucyjne rozszerzone i przyczynowe. Do wyniku z nich jednej z nich aplikowana jest funkcja sigmoidalna a do drugiej tangens hiperboliczny, a następnie obliczany jest iloczyn. W ten sposób sieć może nauczyć się zapamiętywać lub usuwać z pamięci pewne cechy. Budowa bloku wykorzystanego w tej sieci jest podobna do tego przedstawionego na rysunku \ref{fig:wavenet_block}, z tą różnicą że warstwy konwolucyjne są aplikowane do danych oryginalnych i takich z odwróconym wymiarem czasu. Wyniki te są następnie konkatenowane. Jądro w tych warstwach ma rozmiar 2,a rozszerzenia przyjmują wartości od 1 do 128 zatem pole receptywne ma tą samą wielkość co w poprzednim modelu. Tak samo jak w poprzednim modelu dane wyjściowego każdego z bloków są sumowane i przekazywane do warstwy gęsto połączonej i następnie do dekodera CTC.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{resnetWavenet}
	\caption{Architektura modelu ResNet + WaveNet}
\end{figure}

\subsection{DenseNet + WaveNet}

W tym modelu do ekstrakcji cech lokalnych wykorzystano model wzorowany na sieci DenseNet\cite{denseNet}. Architektura tej została przedstawiona na rysunku \ref{fig:denseNet}. Składa się on z 30 warstw konwolucyjnych z \textit{batch normalization} i funkcją aktywacyjną ReLU. Każda z warstw oblicza 12 cech i konkatenuje je z danymi wejściowymi, które otrzymała. Po 30 warstwach zostaje zatem obliczone 360 cech. Po nich następuje warstwa która zmniejsza liczbę cech do 256 poprzez obliczenie kombinacji cech otrzymanych na wejściu. Dalsza część sieci jest analogiczna do poprzedniego modelu
. 
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{denseWaveNet}
	\caption{Architektura modelu DenseNet + WaveNet}
\end{figure}

\section*{Proces uczenia}

Do uczenia sieci wykorzystano algorytm optymalizacyjny Adam\cite{adam}, z \textit{learning rate} równym 0.001. Optymalizowano funkcję straty CTC\cite{ctc}. Sieć była uczona przez maksymalnie 10 epok - proces uczenia był przerywany jeśli po 2 kolejnych epok wartość funkcji straty nie zmniejszała się. Do uczenia wykorzystano procesor graficzny GEFORCE GTX 1080.

\section*{Składanie odczytów}

Podczas dokonywania predykcji długi odczyt sygnału dzielony jest na częściowo pokrywające się ze sobą fragmenty od długości 300 próbek. Każdy taki fragment pokrywa się z następnym w 90\%, to znaczy fragmenty zaczynają się od próbki sygnału o indeksach 0, 30, 60, 90 itd. Uzyskuje się w ten sposób dziesięciokrotne pokrycie krótkimi odczytami. Z racji że kolejność krótkich odczytów w konsensusie jest znana utworzenie go jest łatwe. Uzyskuje się go przez znalezienie najdłuższych zachodzących na siebie fragmentów sąsiadujących odczytów. W ten sposób zliczane są wystąpienia nukleotydów na każdej pozycji i wybierane te, które pojawiają się najczęściej.

\begin{figure}[h!]
	\centering
	\includegraphics{assembly}
	\caption{Wizualizacja składanie krótkich odczytów w długi.}
\end{figure}

\section*{Szczegóły techniczne implementacji}

Modele sieci neuronowych zostały zaimplementowane w języku Python, przy użyciu biblioteki TensorFlow\cite{tensorflow} wersja r1.15. Aplikacja obługuje odczyty w formacie fast5, będącym modyfikacją formatu HDF5 opracowaną przez Oxford Nanopore Technologies do przechowywania danych z sekwencjonowania MinIONem. Obsługiwane są również odczyty w formacie tekstowym wykorzystanym w jednym z użytych zbiorów danych\cite{chironData}. Sekwencje będące wynikiem basecallingu zapisywane są w formacie FASTA. Jest to format tekstowy służący do reprezentowania sekwencji nukleotydów lub białek, szeroko stosowany w bioinformatyce.

\chapter*{Ewaluacja modeli i analiza wyników}

\section{Zbiory danych}

Do oceny jakości modeli wykorzystano zbiory danych zawierające po 2000 odczytów. Pochodziły one z sekwencjonowania próbek \textit{e. coli} i \textit{lambda phage}\cite{chironData}.

\section{Wykorzystane metryki}

Aby ocenić wynik basecallingu na krótkich fragmentach sygnału (300 próbek) obliczono liczbę insercji, substytucji i delecji potrzebnych do przetransformowania wyniku dekodowania CTC do sekwencji DNA, która faktycznie odpowiada fragmentowi sygnału dla którego dokonano basecallingu. Obliczona wartość jest dzielona przez liczbę baz w oczekiwanej sekwencji w celu znormalizowania miary. Otrzymuje się w ten sposób znormalizowany dystans edycji pomiędzy wynikiem basecallingu a fragmentem prawdziwej sekwencji DNA. Miara ta ocenia jakość krótkich fragmentów DNA będących wynikiem bezpośrednio otrzymywanym z modelu.

Z fragmentów DNA otrzymywanych na wyjściu dekodera tworzony jest konsensus odpowiadający sekwencji DNA pewnego odczytu. Sekwencje te są mapowane na odpowiedni genom referencyjny przy użyciu programu minimap2\cite{minimap}. Otrzymane w ten sposób pliki BAM zostały przeanalizowane programem japsa\footnote{https://github.com/mdcao/japsa}, który oblicza liczbę delecji, substytucji i insercji pomiędzy pomiędzy sekwencją wejściowa i fragmentem genomu referencyjnego na, który została zmapowana. Procent delecji, insercji i substytucji obliczony jest przez iloraz liczby tych operacji i długości fragmentu genomu referencyjnego, na który została zmapowana sekwencja. Stopień identyczności pomiędzy sekwencjami jest zdefiniowany jako $1-procent_delecji-procent_substytucji$, a procent błędów jako $procent_insercji + procent_delecji + procent_substytucji$.

Aby sprawdzić jakość konsensusu powstałego z otrzymanych odczytów wykorzystano assembler Rebaler\footnote{https://github.com/rrwick/Rebaler}. Zbudowano przy jego pomocy kontigi z odczytów z sekwencjonowania \textit{lambda phage}. Kontigi zostały podzielone na fragmenty zawierające 10 tysięcy par zasad i zmapowane na genom referencyjny przy pomocy minimap2. Następnie obliczono średni stopień identyczności pomiędzy zmapowanymi sekwencjami i genomem referencyjnym. Obliczono względną długość otrzymanego konsensusu względem genomu referencyjnego. Taka procedura sprawdzenia jakości konsensusu pojawia się w publikacjach porównujących basecallery\cite{wick}\cite{chiron}.

\section*{Ewaluacja testowanych architektur sieci}

\subsection{Proces uczenia}

\begin{figure}[h!]
	\centering
	\includegraphics{training_loss}
	\caption{Wartości funkcji straty podczas procesu uczenia poszczególnych modeli.}
	\label{fig:training}
\end{figure}

Na wykresie \ref{fig:training} przedstawiono wartość funkcji straty osiągane przez modele w trakcie procesu uczenia. W przypadku wszystkich architektur strata spada szybko podczas pierwszej epoki, a potem zmienia się już tylko nieznacznie, Przez cały proces uczenia najmniejszą wartość funkcji straty notują modele oparte o sieci rekurencyjne. 

\begin{figure}[h!]
	\centering
	\includegraphics{val_distance}
	\caption{Znormalizowany dystans edycji liczony na zbiorze treningowym końcu każdej epoki.}
	\label{fig:val_distance}
\end{figure}
	
Podobnie podczas uczenia zachowuje się dystans edycji (wykres \ref{fig:val_distance}. W przypadku modelu DenseNet + WaveNet zwiększa się on podczas ostatniej epoki, co może świadczyć o przetrenowaniu modelu.

\subsection{Dystans edycji krótkich fragmentów}

Podczas basecallingu zbioru testowego obliczono znormalizowany dystans edycji pomiędzy wynikiem predykcji na fragmentach sygnału po 300 próbek, a sekwencjom DNA odpowiadających temu sygnałowi. Wyniki przedstawiono w tabeli \ref{table:edit_distance}. Wyniki te są bardzo podobne do tych uzyskanych na zbiorze treningowym - najlepsze osiągane są przez modele wykorzystujące sieci rekurencyjne.

\begin{table}[]
	\centering
	\begin{tabular}{ll}
		Model            & \begin{tabular}[c]{@{}l@{}}Znormalizowany \\ \\ Dystans Edycji\end{tabular} \\
		ResNet+LSTM      & 0.1544                                                                      \\
		ResNet+TCN       & 0.1853                                                                      \\
		ResNet+WaveNet   & 0.2040                                                                      \\
		DenseNet+WaveNet & 0.1879                                                                     
	\end{tabular}
	\caption{\label{}Znormalizowany dystans edycji dla krótkich fragmentów sygnału obliczony na zbiorze testowym.}
	\label{table:edit_distance}
\end{table}

\subsection{Ocena jakości odczytów}

Sekwencje otrzymane z basecallingu całych odczytów zostały zmapowane na genom referencyjny celu zbadania ich jakości. Wyniki zostały przedstawione w tabelach \ref{table:lambda} i \ref{table:ecoli}. Wszystkie modele produkowały trafniejsze odczyty dla dany \textit{e. coli}. Jakość odczytów jest powiązana z jakością predykcji dla krótkich fragmentów - modele, których wyniki dla krótkich fragmentów osiągały niższy znormalizowany dystans edycji produkowały odczyty o wyższym stopniu identyczności i niższym procencie błędów. Zatem ponownie modele wykorzystujące rekurencyjne sieci neuronowe osiągały lepsze wyniki niż oparte wyłącznie o sieci konwolucyjne.

\begin{table}[]
		\centering
	\begin{tabular}{llllll}
		& \begin{tabular}[c]{@{}l@{}}Procent\\ delecji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent\\ insercji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent \\ substytucji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Stopień\\ identyczności\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent \\ błędów\end{tabular} \\
		ResNet+LSTM      & 6.30\%                                                    & 2.26\%                                                     & 4.44\%                                                         & 89.26\%                                                         & 13.00\%                                                   \\
		ResNet+TCN       & 7.02\%                                                    & 3.00\%                                                     & 6.41\%                                                         & 86.57\%                                                         & 16.43\%                                                   \\
		ResNet+WaveNet   & 9.18\%                                                    & 1.82\%                                                     & 5.74\%                                                         & 85.09\%                                                         & 16.74\%                                                   \\
		DenseNet+WaveNet & 7.40\%                                                    & 2.56\%                                                     & 5.78\%                                                         & 86.83\%                                                         & 15.74\%                                                  
	\end{tabular}
	\caption{\label{}Miary jakości odczytów otrzymanych z poszczególnych modeli dla sygnału z sekwencjonowania \textit{lambda phage}}
	\label{table:lambda}
\end{table}

\begin{table}[]
	\centering
	\begin{tabular}{llllll}
		& \begin{tabular}[c]{@{}l@{}}Procent\\ delecji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent\\ insercji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent \\ substytucji\end{tabular} & \begin{tabular}[c]{@{}l@{}}Stopień\\ identyczności\end{tabular} & \begin{tabular}[c]{@{}l@{}}Procent \\ błędów\end{tabular} \\
		ResNet+LSTM      & 5.42\%                                                    & 2.20\%                                                     & 4.20\%                                                         & 90.39\%                                                         & 11.82\%                                                   \\
		ResNet+TCN       & 6.11\%                                                    & 2.94\%                                                     & 5.94\%                                                         & 87.95\%                                                         & 13.99\%                                                   \\
		ResNet+WaveNet   & 8.15\%                                                    & 1.85\%                                                     & 5.59\%                                                         & 86.26\%                                                         & 15.59\%                                                   \\
		DenseNet+WaveNet & 6.57\%                                                    & 2.52\%                                                     & 5.67\%                                                         & 87.76\%                                                         & 13.76\%                                                  
	\end{tabular}
	\caption{\label{}Miary jakości odczytów otrzymanych z poszczególnych modeli dla sygnału z sekwencjonowania \textit{e. coli}}
	\label{table:ecoli}
\end{table}

\subsection{Ocena jakości konsensusu}

W tabeli \ref{table:consensus} zaprezentowano jakośc konsensusów otrzymanych z odczytów. Wyniki dla każdego z modelu zawierają błędy, częstotliwość ich występowania jest powiązana z iloscią błędów w odczytach, z których powstały kontigi. Zatem wszystkie modele popełniają systematyczne błędy, które zostają odzwierciedlone w konsensusie.

\begin{table}[]
	\begin{tabular}{lll}
		& \begin{tabular}[c]{@{}l@{}}Mediana stopnia\\ identycznośći\end{tabular} & \begin{tabular}[c]{@{}l@{}}Mediana długości\\ względem referencji\end{tabular} \\
		ResNet+LSTM      & 99.67\%                                                                 & 99.69\%                                                                        \\
		ResNet+TCN       & 99.55\%                                                                 & 99.75\%                                                                        \\
		ResNet+WaveNet   & 98.94\%                                                                 & 99.02\%                                                                        \\
		DenseNet+WaveNet & 99.43\%                                                                 & 99.55\%                                                                       
	\end{tabular}
	\caption{\label{}Miary jakości konsensusu otrzymanych z poszczególnych modeli dla sygnału z sekwencjonowania \textit{lambda phage}}
	\label{table:consensus}
\end{table}

\chapter{Wnioski}
% -------------------- 6. Bibliografia -----------------------
% Bibliografia leksykograficznie wg nazwisk autorów
% Dla ambitnych - można skorzystać z BibTeX-a

\begin{thebibliography}{20}%jak ktoś ma więcej książek, to niech wpisze większą liczbę
% \bibitem[numerek]{referencja} Autor, \emph{Tytuł}, Wydawnictwo, rok, strony
% cytowanie: \cite{referencja1, referencja 2,...}
\bibitem{genome}James Fraser, Iain Williamson, Wendy A. Bickmore,  Josée Dostie, An Overview of Genome Organization and How We Got There: from FISH to Hi-C
\bibitem{cancerImmunity}Finotello, F., Rieder, D., Hackl, H. et al. Next-generation computational tools for interrogating cancer immunity. Nat Rev Genet 20, 724–746 (2019) doi:10.1038/s41576-019-0166-7
\bibitem{cancerNonCoding}Khurana, E., Fu, Y., Chakravarty, D. et al. Role of non-coding sequence variants in cancer. Nat Rev Genet 17, 93–108 (2016) doi:10.1038/nrg.2015.17
\bibitem{adaptation} Marciniak, S., Perry, G. Harnessing ancient genomes to study the history of human adaptation. Nat Rev Genet 18, 659–674 (2017) doi:10.1038/nrg.2017.65
\bibitem{personalizedMedicine} Ashley, E. Towards precision medicine. Nat Rev Genet 17, 507–522 (2016) doi:10.1038/nrg.2016.86
\bibitem{prenatal} Chiu, R. W. et al. Noninvasive prenatal diagnosis of fetal chromosomal aneuploidy by massively parallel genomic sequencing of DNA in maternal plasma. Proc. Natl Acad. Sci. USA105, 20458–20463 (2008)
\bibitem{diagnosis} Choi, M. et al. Genetic diagnosis by whole exome capture and massively parallel DNA sequencing. Proc. Natl Acad. Sci. USA106, 19096–19101 (2009).
\bibitem{sequencingComparison} Liu L, Li Y, Li S, Hu N, He Y, Pong R, Lin D, Lu L, Law M (1 January 2012). "Comparison of Next-Generation Sequencing Systems". Journal of Biomedicine and Biotechnology. 2012: 251364. doi:10.1155/2012/251364.
\bibitem{sequencing} Jay Shendure Shankar Balasubramanian, George M. Church, Walter Gilbert, Jane Rogers, Jeffery A. Schloss, Robert h. Waterston DNA sequencing at 40: past, present and future
\bibitem{sequencingTechnologies} James M.Heather, Benjamin Chain The sequence of sequencers: The history of sequencing DNA
\bibitem{refGenome} Susan Fairley, Ernesto Lowy-Gallego, Emily Perry, Paul Flicek, The International Genome Sample Resource (IGSR) collection of open human genomic variation resources, Nucleic Acids Research.
\bibitem{nanoporeHuman} Miten Jain, Sergey Koren, Karen H Miga, Josh Quick, Arthur C Rand, Thomas A Sasani, John R Tyson, Andrew D Beggs, Alexander T Dilthey, Ian T Fiddes, Sunir Malla, Hannah Marriott, Tom Nieto, Justin O'Grady, Hugh E Olsen, Brent S Pedersen, Arang Rhie, Hollian Richardson, Aaron R Quinlan, Terrance P Snutch, Louise Tee, Benedict Paten, Adam M Phillippy, Jared T Simpson, Nicholas J Loman \& Matthew Loose. Nanopore sequencing and assembly of a human genome with ultra-long reads. Nature Biotechnology
\bibitem{ebola} Hoenen T, Groseth A, Rosenke K, et al. Nanopore Sequencing as a Rapidly Deployable Ebola Outbreak Tool. Emerg Infect Dis. 2016;22(2):331–334. doi:10.3201/eid2202.151796
\bibitem{deepNano}Boza, V., Brejova, B. \& Vinar, T. Deepnano: Deep recurrent neural networks for base calling in minion nanopore reads.PloS one12, e0178751 (2017).
\bibitem{basecrawler}Stoiber, M. \& Brown, J. Basecrawller: Streaming nanopore basecalling directly from raw signal.bioRxiv133058 (2017).
\bibitem{chiron} Haotian Teng, Minh Duc Cao, Michael B Hall, Tania Duarte, Sheng Wang, Lachlan J M Coin, Chiron: translating nanopore raw signal directly into nucleotide sequence using deep learning, GigaScience, Volume 7, Issue 5, May 2018, giy037, https://doi.org/10.1093/gigascience/giy037
\bibitem{wick}Wick, R.R., Judd, L.M. \& Holt, K.E. Performance of neural network basecalling tools for Oxford Nanopore sequencing. Genome Biol 20, 129 (2019) doi:10.1186/s13059-019-1727-y
\bibitem{snp}Pightling AW, Pettengill JB, Luo Y, Baugher JD, Rand H, Strain E. Interpreting whole-genome sequence analyses of foodborne bacteria for regulatory applications and outbreak investigations. Front Microbiol. 2018; 9:1–13. https://doi.org/10.3389/fmicb.2018.01482.
\bibitem{nanoraw}Stoiber, M.H. et al. De novo Identification of DNA Modifications Enabled by Genome-Guided Nanopore Signal Processing. bioRxiv (2016).
\bibitem{chironData} Teng H; Cao MD; Hall MB; Duarte T; Wang S; Coin LJM (2018): Supporting data for "Chiron: Translating nanopore raw signal directly into nucleotide sequence using deep learning" GigaScience Database.
\bibitem{resnet} K. He, X. Zhang, S. Ren and J. Sun, “Deep Residual Learning for Image Recognition,” in CVPR, 2016.
\bibitem{preResnet}K. He, X. Zhang, S. Ren, and J. Sun. Identity Mappings in Deep Residual Networks. arXiv preprint arXiv:1603.05027v3,2016.
\bibitem{cnn}Yann LeCunn, Leon Bottou, Yoshua Bengio, Patrick Haffner, Gradient-Based Learning Applied to Document Recognition, 1998
\bibitem{vgg}Very Deep Convolutional Networks for Large-Scale Image Recognition Karen Simonyan, Andrew Zisserman
\bibitem{cnnVideo} Exploiting Image-trained CNN Architectures for Unconstrained Video Classification Shengxin Zha, Florian Luisier, Walter Andrews, Nitish Srivastava, Ruslan Salakhutdinov
\bibitem{cnnAudio} Environmental Sound Classification With Convolutional Neural Networks Karol J. Piczak
\bibitem{difficulty}Understanding the difficulty of training deep feedforward neural networks Xavier Glorot, Yoshua Bengio
\bibitem{segmentation} Recent progress in semantic image segmentation Xiaolong Liu, Zhidong Deng, Yuhan Yang
\bibitem{sequenceModelling} An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling Shaojie Bai, J. Zico Kolter, Vladlen Koltun
\bibitem{bn}Sergey Ioffe, Christian Szegedy Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift https://arxiv.org/abs/1502.03167
\bibitem{fcn}Fully Convolutional Networks for Semantic Segmentation Jonathan Long, Evan Shelhamer, Trevor Darrell
\bibitem{DSN}Deeply-Supervised Nets Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu
\bibitem{denseNet}Densely Connected Convolutional Networks Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger
\bibitem{rnn} A., Servan-Schreiber, D., and McClelland, J. L. Finite-state automata and simple recurrent networks Neural Computation, pp , 1:372-381.
\bibitem{rnnDifficulty} Y. Bengio, P. Simard, P. Frasconi Learning long-term dependencies with gradient descent is difficult, IEEE Transactions on Neural Networks (Volume: 5 ,Issue: 2 , March 1994)
\bibitem{lstmSpeech} Alex Graves, Navdeep Jaitly Towards End-to-End Speech Recognition with Recurrent Neural Networks
\bibitem{lstmVideo} Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko,  Sequence to Sequence -- Video to Text
\bibitem{lstmHandwriting} Alex Graves, Jurgen Schmidhuber Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks
\bibitem{lstmTranslation} Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
\bibitem{fcnTranslation}Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Prediction Maha Elbayad, Laurent Besacier, Jakob Verbeek
\bibitem{lstm} Sepp Hochreiter; Jürgen Schmidhuber Long short-term memory. Neural Computation
\bibitem{tcn} Nal Kalchbrenner  Lasse Espeholt  Karen Simonyan  Aaron van den Oord  Alex Graves  Koray Kavukcuoglu Neural Machine Translation in Linear Time
\bibitem{wavenet}Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu WaveNet: A Generative Model for Raw Audio
\bibitem{wavenetReview} Boilard, Jonathan \& Gournay, Philippe \& Lefebvre, R.. (2019). A Literature Review of WaveNet: Theory, Application and Optimization. 
\bibitem{ctc}Alex Graves, Santiago Fernandez, Faustino Gomez, Jurgen Schmidhuber, Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks
\bibitem{adam}Kingma, D.P., \& Ba, J. (2014). Adam: A Method for Stochastic Optimization. CoRR, abs/1412.6980.
\bibitem{tensorflow}Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.
\bibitem{minimap} Li, H. (2018). Minimap2: pairwise alignment for nucleotide sequences. Bioinformatics, 34:3094-3100. doi:10.1093/bioinformatics/bty191
\end{thebibliography}

\thispagestyle{empty}
\pagenumbering{gobble}



% --- 7. Wykaz symboli i skrótów - jeśli nie ma, zakomentować

% ----- 8. Spis rysunków - jeśli nie ma, zakomentować --------
\listoffigures
\thispagestyle{empty}
Jak nie występują, usunąć.


% ------------ 9. Spis tabel - jak wyżej ------------------
\renewcommand{\listtablename}{Spis tabel}
\listoftables
\thispagestyle{empty}
Jak nie występują, usunąć.


% 10. Spis załączników - jak nie ma załączników, to zakomentować lub usunąć

\thispagestyle{empty}


\end{document}
